{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras train for speech.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villasen/colab_notebooks/blob/master/Keras_train_for_speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjlCIz9y8NKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/villasen/speech-numpy.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw3IpreVLIgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/villasen/Speech-tiny.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc99ICZWzk9D",
        "colab_type": "code",
        "outputId": "e295b7d6-6e4a-4c9d-88c1-d7144acd557d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow\n",
        "import scipy\n",
        "import os, shutil\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from six.moves import urllib\n",
        "import sys\n",
        "import tarfile"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLXjMkQtz-RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dest_directory = \"/content/speech_target/\"\n",
        "data_url='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "DATA_PATH= '/content/Speech-tiny/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTw2j5PBDhGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -al speech-numpy/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELc68NSuL67M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -al speech_target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUGyYh7GMB_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r sample_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m0d1WKMDoFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -r speech-numpy/README.md\n",
        "#!rm -r speech-numpy/.ipynb_checkpoints/\n",
        "!rm -r speech-numpy/.git/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm7eY_jazNYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_url='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "os.makedirs(dest_directory)\n",
        "filename = data_url.split('/')[-1]\n",
        "filepath = os.path.join(dest_directory, filename)\n",
        "\n",
        "print(filename)\n",
        "print(filepath)\n",
        " \n",
        "def _progress(count, block_size, total_size):\n",
        "    sys.stdout.write('\\r>> Downloading %s %.1f%%' % \\\n",
        "            (filename, float(count * block_size) / float(total_size) * 100.0)) \n",
        "    sys.stdout.flush()\n",
        "\n",
        "urban_dataset_dir = '/content/combined_datasets/' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjymcs8vJlYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/target_npy_files/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA1SHcVy0r8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  \n",
        "def maybe_download_and_extract_dataset(data_url, dest_directory):\n",
        "    \"\"\"Download and extract data set tar file.\n",
        "    If the data set we're using doesn't already exist, this function\n",
        "    downloads it from the TensorFlow.org website and unpacks it into a\n",
        "    directory.\n",
        "    If the data_url is none, don't download anything and expect the data\n",
        "    directory to contain the correct files already.\n",
        "    Args:\n",
        "      data_url: Web location of the tar file containing the data set.\n",
        "      dest_directory: File path to extract data to.\n",
        "    \"\"\"\n",
        "    if not data_url:\n",
        "      return\n",
        "    if not os.path.exists(dest_directory):\n",
        "      os.makedirs(dest_directory)\n",
        "    filename = data_url.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "    print(filepath)\n",
        "    if not os.path.exists(filepath):\n",
        "\n",
        "      def _progress(count, block_size, total_size):\n",
        "        sys.stdout.write(\n",
        "            '\\r>> Downloading %s %.1f%%' %\n",
        "            (filename, float(count * block_size) / float(total_size) * 100.0))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "      try:\n",
        "        filepath, _ = urllib.request.urlretrieve(data_url, filepath, _progress)\n",
        "      except:\n",
        "        tf.logging.error('Failed to download URL: %s to folder: %s', data_url,\n",
        "                         filepath)\n",
        "        tf.logging.error('Please make sure you have enough free space and'\n",
        "                         ' an internet connection')\n",
        "        raise\n",
        "      print()\n",
        "      statinfo = os.stat(filepath)\n",
        "      tf.logging.info('Successfully downloaded %s (%d bytes)', filename,\n",
        "                      statinfo.st_size)\n",
        "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ZZkMggaiVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF9If1_90vzr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e370653b-43b6-4238-ac7f-0ff3a8d4ef37"
      },
      "source": [
        "maybe_download_and_extract_dataset( data_url, dest_directory)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/speech_target/speech_commands_v0.02.tar.gz\n",
            ">> Downloading speech_commands_v0.02.tar.gz 100.0%\n",
            "INFO:tensorflow:Successfully downloaded speech_commands_v0.02.tar.gz (2428923189 bytes)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4jURFILmxKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls speech_target/ -al"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8yDReprIWOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = os.listdir('speech-numpy/')\n",
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUjMfCPbw4RM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c19bca6-9a00-4f6f-a529-17a689243d68"
      },
      "source": [
        "!rm -r /content/test_single_file/"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDar6P1BOO7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_single_file = '/content/test_single_file/'\n",
        "test_single_file = '/content/test_single_file/'\n",
        "os.mkdir(test_single_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#!mv speech_target/bed/00176480_nohash_0.wav test_single_file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT3s-GvC0e5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJc5ohRF0wcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('/content/test_single_file/cat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEbFdz_B1K42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/Speech-tiny/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GROsioDvwMGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.chdir(test_single_file)\n",
        "#os.mkdir('bed')\n",
        "!mv /content/Speech-tiny/learn/234d6a48_nohash_1.wav /content/test_single_file/learn/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pkC0OzfMFBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "target_dir = '/content/target_npy_files'\n",
        "target_model = '/content/target_model/'\n",
        "#DATA_PATH = \"small-urban-sound-dataset/tiny-dataset/\"\n",
        "#DATA_PATH = \"small-urban-sound-dataset/combined_datasets/\"\n",
        "DATA_PATH= '/content/Speech-tiny/'\n",
        "os.mkdir(target_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def wav2mfcc(file_path, max_pad_len):\n",
        "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=640, hop_length=320)\n",
        "    pad_width = max_pad_len - mfcc.shape[1]\n",
        "    if pad_width < 0: \n",
        "      print(mfcc.shape[1])\n",
        "      print(pad_width)\n",
        "      print(\"error in \"+ file_path)\n",
        "    \n",
        "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    return mfcc  \n",
        "\n",
        "  \n",
        "  \n",
        "def get_labels(path):\n",
        "    labels = os.listdir(path) \n",
        "    label_indices = np.arange(0, len(labels))\n",
        "    return labels, label_indices, to_categorical(label_indices)  \n",
        "\n",
        "  \n",
        "def save_data_to_array(path=DATA_PATH, max_pad_len=51):\n",
        "    labels, _, _ = get_labels(path)\n",
        "\n",
        "    for label in labels:\n",
        "        # Init mfcc vectors\n",
        "        mfcc_vectors = []\n",
        "\n",
        "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "        for wavfile in wavfiles:\n",
        "            if label == '_background_noise_' : break\n",
        "            mfcc = wav2mfcc(wavfile, max_pad_len=max_pad_len)\n",
        "            \n",
        "            mfcc_vectors.append(mfcc)\n",
        "        np.save('/content/target_npy_files/' + label + '.npy', mfcc_vectors)\n",
        "        print(label)\n",
        "        print(len(mfcc_vectors))\n",
        "          \n",
        "\n",
        "def get_train_test(split_ratio=0.9, random_state=42):\n",
        "    # Get available labels\n",
        "    labels, indices, _ = get_labels(DATA_PATH)\n",
        "    print(labels)\n",
        "    # Getting first arrays\n",
        "    X = np.load('/content/target_npy_files/' + labels[0] + '.npy')\n",
        "    #X = np.load('/content/speech-numpy/' + labels[0])\n",
        "    y = np.zeros(X.shape[0])\n",
        "\n",
        "    \n",
        "    # Append all of the dataset into one single array, same goes for y\n",
        "    for i, label in enumerate(labels[1:]):\n",
        "        x = np.load('/content/target_npy_files/' + label + '.npy')\n",
        "        #x = np.load('/content/speech-numpy/' + label)\n",
        "        \n",
        "        X = np.vstack((X, x))\n",
        "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
        "        \n",
        "    assert X.shape[0] == len(y)\n",
        "\n",
        "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr_vtdgiamjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#target_dir = '/content/target_npy_files'\n",
        "target_dir = /content/target_unknown_npy\n",
        "target_model = '/content/target_model/'\n",
        "#DATA_PATH = \"small-urban-sound-dataset/tiny-dataset/\"\n",
        "#DATA_PATH = \"small-urban-sound-dataset/combined_datasets/\"\n",
        "DATA_PATH= '/content/speech_target/'\n",
        "os.mkdir(target_dir)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def wav2mfcc(file_path, max_pad_len):\n",
        "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=640, hop_length=320)\n",
        "    pad_width = max_pad_len - mfcc.shape[1]\n",
        "    if pad_width < 0: \n",
        "      print(mfcc.shape[1])\n",
        "      print(pad_width)\n",
        "      print(\"error in \"+ file_path)\n",
        "    \n",
        "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    return mfcc  \n",
        "\n",
        "  \n",
        "  \n",
        "def get_labels(path):\n",
        "    labels = os.listdir(path) \n",
        "    label_indices = np.arange(0, len(labels))\n",
        "    return labels, label_indices, to_categorical(label_indices)  \n",
        "\n",
        "  \n",
        "def save_data_to_array(path=DATA_PATH, max_pad_len=51):\n",
        "    labels, _, _ = get_labels(path)\n",
        "\n",
        "    for label in labels:\n",
        "        # Init mfcc vectors\n",
        "        mfcc_vectors = []\n",
        "\n",
        "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "        for wavfile in wavfiles:\n",
        "            if label == '_background_noise_' : break\n",
        "            mfcc = wav2mfcc(wavfile, max_pad_len=max_pad_len)\n",
        "            \n",
        "            mfcc_vectors.append(mfcc)\n",
        "        np.save('/content/target_npy_files/' + label + '.npy', mfcc_vectors)\n",
        "        print(label)\n",
        "        print(len(mfcc_vectors))\n",
        "\n",
        "        \n",
        "        \n",
        "def save_data_to_unknown_array(path=DATA_PATH, max_pad_len=51):\n",
        "  labels, _, _ = get_labels(path)\n",
        "  \n",
        "  for label in labels:\n",
        "    mfcc_vectors = []\n",
        "    \n",
        "    wavefile = [path + label + '/' + wavfile for wavefile in os.listdir(path + '/' + label)]\n",
        "    \n",
        "\n",
        "def get_train_test(split_ratio=0.9, random_state=42):\n",
        "    # Get available labels\n",
        "    labels, indices, _ = get_labels(DATA_PATH)\n",
        "    print(labels)\n",
        "    # Getting first arrays\n",
        "    X = np.load('/content/target_npy_files/' + labels[0] + '.npy')\n",
        "    #X = np.load('/content/speech-numpy/' + labels[0])\n",
        "    y = np.zeros(X.shape[0])\n",
        "\n",
        "    \n",
        "    # Append all of the dataset into one single array, same goes for y\n",
        "    for i, label in enumerate(labels[1:]):\n",
        "        x = np.load('/content/target_npy_files/' + label + '.npy')\n",
        "        #x = np.load('/content/speech-numpy/' + label)\n",
        "        \n",
        "        X = np.vstack((X, x))\n",
        "        y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
        "        \n",
        "    assert X.shape[0] == len(y)\n",
        "\n",
        "    return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ8zjMUgEJBT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pebkUlJWxXTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.listdir('Speech-tiny/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhHJpQ2ox-0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels, indices, _ = get_labels('Speech-tiny/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql7omP_myNmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL8yJGZk9RKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_data_to_array(path=DATA_PATH, max_pad_len=51)    \n",
        "X_train, X_test, y_train, y_test = get_train_test()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 10, 51, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 10, 51, 1)\n",
        "y_train_hot = to_categorical(y_train)\n",
        "y_test_hot = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIo6kgbPNmkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r target_model/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNgVTpaS9A57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating Keras sequential model\n",
        "#bn = 1\n",
        "target_model = '/content/target_model/'\n",
        "os.mkdir(target_model)\n",
        "BN=True\n",
        "model = models.Sequential()\n",
        "\n",
        "def dscnn_train():\n",
        "  # 1\n",
        "      model.add(layers.Conv2D(64, (4,10), strides=(2,2), padding='same', activation='relu', \\\n",
        "                #input_shape=(10, 49, 1)))\n",
        "                input_shape=(10,51,1)))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                                center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                                gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                                moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                                gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      \n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      # 2\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))  \n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "     # model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "     # model.add(layers.Dropout(0.5))\n",
        "      # 3\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      \n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "     # model.add(layers.Dropout(0.5))\n",
        "      \n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "\n",
        "      # 4\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "\n",
        "      # 5\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:      \n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "\n",
        "\n",
        "      \n",
        "           \n",
        "      \n",
        "# Final layer\n",
        "      \n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.AveragePooling2D(pool_size=(5, 25), strides=(2,2), padding='valid', data_format=None))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.Flatten(data_format=None))\n",
        "     # model.add(layers.Dropout(0.5))\n",
        "      model.add(layers.Dense(64, activation='relu'))\n",
        "      model.add(layers.Dropout(0.5))\n",
        "      #model.add(layers.Dense(12, activation='softmax'))\n",
        "      model.add(layers.Dense(5, activation='softmax'))\n",
        "      #model.add(layers.Dropout(0.5))\n",
        "      # Compilation step to choose loss function, optimizer and metric\n",
        "      # Configuring the learning process\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      #model.fit()\n",
        "      history = model.fit(X_train, y_train_hot, batch_size=100, epochs=200, verbose=1, validation_data=(X_test, y_test_hot))\n",
        "      \n",
        "      #plt.plot(history.history['acc'])\n",
        "      #Restarts layer sequence number \n",
        "      #K.clear_session()\n",
        "\n",
        "      acc = history.history['acc']\n",
        "      val_acc = history.history['val_acc']\n",
        "      loss = history.history['loss']\n",
        "      val_loss = history.history['val_loss']\n",
        "      epochs = range(1, len(acc) + 1)\n",
        "      \n",
        "      plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "      plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "      plt.title('Training and validation accuracy')\n",
        "      plt.legend()\n",
        "\n",
        "      plt.figure()\n",
        "\n",
        "      plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "      plt.plot(epochs,val_loss, 'b', label='Validation loss')\n",
        "      plt.title('Training and validation loss')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "      \n",
        "      \n",
        "      model.save('/content/target_model/speech_recognition.h5')\n",
        "  \n",
        "dscnn_train()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOYhIaeZihsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mfcc_bed = wav2mfcc('test_single_file/bed/0135f3f2_nohash_0.wav', max_pad_len=51)\n",
        "mfcc_cat = wav2mfcc('test_single_file/cat/069ab0d5_nohash_0.wav', max_pad_len=51)\n",
        "mfcc_dog = wav2mfcc('test_single_file/dog/012c8314_nohash_0.wav', max_pad_len=51)\n",
        "mfcc_learn = wav2mfcc('test_single_file/learn/234d6a48_nohash_1.wav', max_pad_len=51)\n",
        "mfcc_sheila = wav2mfcc('test_single_file/sheila/00970ce1_nohash_0.wav', max_pad_len=51)\n",
        "\n",
        "\n",
        "bed_file = mfcc_bed.reshape(1, 10, 51, 1)\n",
        "cat_file = mfcc_cat.reshape(1, 10, 51, 1)\n",
        "dog_file = mfcc_dog.reshape(1, 10, 51, 1)\n",
        "learn_file = mfcc_learn.reshape(1, 10, 51, 1)\n",
        "sheila_file = mfcc_sheila.reshape(1, 10, 51, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OPigM_Ij6QF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8436975d-c440-42df-a7f1-eb18c56d320d"
      },
      "source": [
        "#prediction = model.predict(np.array(tk.texts_to_sequences(text)))\n",
        "prediction = model.predict(dog_file, batch_size=None, verbose=0, steps=None)\n",
        "class_prediction = prediction.tolist()\n",
        "\n",
        "for record in class_prediction: \n",
        "    #print(record)\n",
        "    record\n",
        "\n",
        "n = len(record)\n",
        "\n",
        "\n",
        "def predict(record, n):\n",
        "\n",
        "  \n",
        "     \n",
        "    max = record[0] \n",
        "    j = 0\n",
        " \n",
        "    for i in range(1, n):\n",
        "        \n",
        "        if record[i] > max: \n",
        "            \n",
        "            max = record[i]\n",
        "            \n",
        "        \n",
        "    index = record.index(max)        \n",
        "    return max, index  \n",
        "  \n",
        "    \n",
        "predicted_class, indice = predict(record,n) \n",
        "\n",
        "\n",
        "print('Predicted word: ', labels[indice])\n",
        "\n"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted word:  dog\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}