{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6words-full-integer-quantization-ESC50-splitted-16KHz.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villasen/colab_notebooks/blob/master/6words_full_integer_quantization_ESC50_splitted_16KHz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixSfN52b0FhH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "f780b519-868f-492e-b892-072a38c2e1cd"
      },
      "source": [
        "!git clone https://github.com/villasen/Sound-Datasets.git\n",
        "!rm -r sample_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Sound-Datasets'...\n",
            "remote: Enumerating objects: 49500, done.\u001b[K\n",
            "remote: Total 49500 (delta 0), reused 0 (delta 0), pack-reused 49500\u001b[K\n",
            "Receiving objects: 100% (49500/49500), 1.97 GiB | 35.97 MiB/s, done.\n",
            "Resolving deltas: 100% (15354/15354), done.\n",
            "Checking out files: 100% (53687/53687), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf9a330N0iOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MFCC=14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JfGzx141sRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9539035e-6f0f-49ba-df04-0f696e159ea2"
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow\n",
        "import scipy\n",
        "import os, shutil\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from six.moves import urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import os.path\n",
        "from os import path\n",
        "from tensorflow.python.ops import io_ops\n",
        "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from numpy  import array\n",
        "from tensorflow.contrib.quantize.python import fold_batch_norms\n",
        "from tensorflow.contrib.quantize.python import quantize\n",
        "from tensorflow.python.framework import ops\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########### FUNCTIONS\n",
        "\n",
        "def urban_wav2mfcc(file_path, max_pad_len):\n",
        "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=MFCC, n_fft=640, hop_length=320)\n",
        "      \n",
        "    pad_width = max_pad_len - mfcc.shape[1]\n",
        "    #print(max_pad_len)\n",
        "    #print(mfcc.shape[1])\n",
        "    #print(pad_width)\n",
        "    if pad_width < 0: \n",
        "      print(mfcc.shape[1])\n",
        "      print(pad_width)\n",
        "      print(\"error in \"+ file_path)\n",
        "    \n",
        "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    return mfcc \n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "def load_wav_file(wav_file):\n",
        "  with tf.Session(graph=tf.Graph()) as sess:\n",
        "      wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
        "      wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
        "      wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)\n",
        "      return sess.run(\n",
        "          wav_decoder,\n",
        "          feed_dict={wav_filename_placeholder: wav_file}).audio.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRwHIPoL1wM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow-gpu\n",
        "# Installing tf version 1.15\n",
        "! pip uninstall -y tensorflow\n",
        "#! pip install -U tf-nightly\n",
        "!pip install tf-nightly==1.15.0.dev20190821\n",
        "!rm -rf /tmp/urban_sound_models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Lew_C017Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_urban_model():\n",
        " return keras.Sequential([\n",
        "      \n",
        "    keras.layers.Conv2D(64, (4,MFCC), strides=(2,2), padding='same', activation='relu', input_shape=(MFCC,51,1), kernel_regularizer=keras.regularizers.l2(0.001)),     \n",
        "\n",
        "# Depthwise layers\n",
        "    keras.layers.Dropout(0.5), \n",
        "    keras.layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "      \n",
        "    keras.layers.AveragePooling2D(pool_size=(5, 25), strides=(2,2), padding='valid', data_format=None),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Flatten(data_format=None),\n",
        "    keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001),  activation='relu'),\n",
        "  \n",
        "      \n",
        "    keras.layers.Dense(6, activation='softmax')     \n",
        "      \n",
        "      \n",
        "  ])  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqEPCQO32Ey-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "outputId": "2f2489de-f094-472e-c25e-d1c72502ab2d"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "#!rm -r _background_noise_\n",
        "\n",
        "MFCC=14\n",
        "\n",
        "DATA_PATH= '/content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/'\n",
        "!ls -al /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/\n",
        "\n",
        "\n",
        "labels = os.listdir('Sound-Datasets/downsampled_ESC-50_splitted_data_set/')\n",
        "print(labels)\n",
        "\n",
        "test_single_file = '/content/test_single_file/'\n",
        "if path.exists('/content/test_single_file/') :\n",
        "    print (\"folder test_single exits, removing\")\n",
        "    !rm -rf /content/test_single_file\n",
        "os.mkdir(test_single_file)\n",
        "\n",
        "target_npy_files = \"/content/target_npy_files/\"\n",
        "if path.exists(\"/content/target_npy_files/\") :\n",
        "    print (\"folder target_npy_files exits, removing\")\n",
        "    !rm -rf /content/target_npy_files\n",
        "os.mkdir(target_npy_files)\n",
        "\n",
        "\n",
        "target_txt_files = \"/content/target_txt_files/\"\n",
        "if path.exists(\"/content/target_txt_files/\") :\n",
        "    print (\"folder target_txt_files folder exits, removing\")\n",
        "    !rm -rf /content/target_txt_files\n",
        "os.mkdir(target_txt_files)\n",
        "\n",
        "\n",
        "\n",
        "#crying baby\n",
        "# if path.exists(\"/content/test_single_file/crying_baby\") :\n",
        "#      print (\"folder test_single_file/crying_baby exits, removing\")\n",
        "#      !rm -rf /content/test_single_file/crying_baby\n",
        "# os.mkdir('/content/test_single_file/crying_baby')\n",
        "\n",
        "# dog\n",
        "if path.exists(\"/content/test_single_file/dog\") :\n",
        "     print (\"folder test_single_file/dog exits, removing\")\n",
        "     !rm -rf /content/test_single_file/dog\n",
        "os.mkdir('/content/test_single_file/dog')\n",
        "\n",
        "\n",
        "\n",
        "# clapping\n",
        "if path.exists(\"/content/test_single_file/clapping\") :\n",
        "     print (\"folder test_single_file/clapping exits, removing\")\n",
        "     !rm -rf /content/test_single_file/clapping\n",
        "os.mkdir('/content/test_single_file/clapping')\n",
        "\n",
        "# door knock\n",
        "if path.exists(\"/content/test_single_file/door_knock\") :\n",
        "    print (\"folder test_single_file/door_knock exits, removing\")\n",
        "    !rm -rf /content/test_single_file/door_knock\n",
        "os.mkdir('/content/test_single_file/door_knock')\n",
        "\n",
        "# clock alarm\n",
        "if path.exists(\"/content/test_single_file/clock_alarm\") :\n",
        "    print (\"folder test_single_file/clock_alarm exits, removing\")\n",
        "    !rm -rf /content/test_single_file/clock_alarm\n",
        "os.mkdir('/content/test_single_file/clock_alarm')\n",
        "\n",
        "# glass_breaking\n",
        "if path.exists(\"/content/test_single_file/glass_breaking\") :\n",
        "    print (\"folder test_single_file/glass_breaking exits, removing\")\n",
        "    !rm -rf /content/test_single_file/glass_breaking\n",
        "os.mkdir('/content/test_single_file/glass_breaking')\n",
        "\n",
        "# fireworks\n",
        "if path.exists(\"/content/test_single_file/fireworks\") :\n",
        "    print (\"folder test_single_file/fireworks exits, removing\")\n",
        "    !rm -rf /content/test_single_file/fireworks\n",
        "os.mkdir('/content/test_single_file/fireworks')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 608\n",
            "drwxr-xr-x 52 root root  4096 Oct 17 02:32 .\n",
            "drwxr-xr-x 13 root root  4096 Oct 17 02:33 ..\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 airplane\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 breathing\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 brushing_teeth\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 can_opening\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 car_horn\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 cat\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 chainsaw\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 chirping_birds\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 church_bells\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 clapping\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 clock_alarm\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 clock_tick\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 coughing\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 cow\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 crackling_fire\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 crickets\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 crow\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 crying_baby\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 dog\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 door_knock\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 door_wood_creacks\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 drinking_sipping\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 engine\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 fireworks\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 footsteps\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 frog\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 glass_breaking\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 handsaw\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 Helicopter\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 hen\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 insects\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 keyboard_typing\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 laughing\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 mouse_click\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 pig\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 pouring_water\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 rain\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 rooster\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 sea_waves\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 sheep\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 siren\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 sneezing\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 snoring\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 thunderstorm\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 toilet_flush\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 train\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 vacuum_cleaner\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 washing_machine\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 water_drops\n",
            "drwxr-xr-x  2 root root 12288 Oct 17 02:32 wind\n",
            "['door_wood_creacks', 'chirping_birds', 'insects', 'sea_waves', 'clock_alarm', 'chainsaw', 'siren', 'frog', 'clapping', 'cow', 'rooster', 'drinking_sipping', 'clock_tick', 'can_opening', 'train', 'crickets', 'washing_machine', 'sheep', 'hen', 'thunderstorm', 'breathing', 'glass_breaking', 'water_drops', 'crackling_fire', 'engine', 'handsaw', 'brushing_teeth', 'laughing', 'car_horn', 'pouring_water', 'sneezing', 'wind', 'cat', 'airplane', 'crow', 'door_knock', 'mouse_click', 'pig', 'fireworks', 'toilet_flush', 'church_bells', 'vacuum_cleaner', 'Helicopter', 'rain', 'snoring', 'keyboard_typing', 'dog', 'crying_baby', 'coughing', 'footsteps']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN7VqLDX6vXk",
        "colab_type": "text"
      },
      "source": [
        "### Test sample 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsZkdzXY6qIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fd97da13-d53c-4550-c507-d9c4667109a3"
      },
      "source": [
        "\n",
        "!mv  /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/crying_baby/A-5-198411-D.wav  /content/test_single_file/crying_baby/\n",
        "!mv  /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/clapping/A-1-105224-A.wav  /content/test_single_file/clapping/\n",
        "!mv  /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/dog/A-1-97392-A.wav  /content/test_single_file/dog/\n",
        "!mv  /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/door_knock/A-1-101336-A.wav  /content/test_single_file/door_knock/\n",
        "!mv  /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/clock_alarm/A-5-210612-A.wav  /content/test_single_file/clock_alarm/\n",
        "!mv  /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/glass_breaking/A-1-84705-A.wav  /content/test_single_file/glass_breaking/\n",
        "!mv  /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/fireworks/A-1-160563-A.wav  /content/test_single_file/fireworks/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot move '/content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/crying_baby/A-5-198411-D.wav' to '/content/test_single_file/crying_baby/': Not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clevdCEI623E",
        "colab_type": "text"
      },
      "source": [
        "### Test sample 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOiX8SOD66_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/crying_baby/C-1-187207-A.wav  /content/test_single_file/crying_baby/\n",
        "!cp -r /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/clapping/E-5-221950-A.wav  /content/test_single_file/clapping/\n",
        "!cp -r /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/door_knock/D-5-250026-B.wav  /content/test_single_file/door_knock/\n",
        "!cp -r /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/clock_alarm/A-3-117793-A.wav  /content/test_single_file/clock_alarm/\n",
        "!cp -r /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/glass_breaking/D-3-216280-A.wav  /content/test_single_file/glass_breaking/\n",
        "!cp -r /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/fireworks/E-5-160614-B.wav  /content/test_single_file/fireworks/\n",
        "!cp -r /content/Sound-Datasets/downsampled_ESC-50_splitted_data_set/dog/C-5-203128-B.wav  /content/test_single_file/dog/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGCyr3j582pI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "60823154-0976-4788-f8d6-2a0a543e76f6"
      },
      "source": [
        "path = DATA_PATH\n",
        "labels = os.listdir(path) \n",
        "print(labels)\n",
        "\n",
        "max_feature = 0\n",
        "for label in labels:\n",
        "  mfcc_vectors=[]\n",
        "   \n",
        "  wavefiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "    \n",
        "  if label=='dog' or label=='clapping' or label=='door_knock' or label=='clock_alarm' \\\n",
        "             or label=='glass_breaking' or label=='fireworks': \n",
        "#  if label=='clapping': \n",
        "    for wavfile in wavefiles:\n",
        "            \n",
        "      max_pad_len = 51\n",
        "      #mfcc = urban_wav2mfcc(wavfile, 51)\n",
        "      wave, sr = librosa.load(wavfile , mono=True, sr=None)\n",
        "      #mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=3200, hop_length=1600)\n",
        "      mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=MFCC, n_fft=640, hop_length=320)\n",
        "      #print(mfcc)\n",
        "      pad_width = max_pad_len - mfcc.shape[1]\n",
        "      #print(max_pad_len)\n",
        "      #print(mfcc.shape[1])\n",
        "      #print(pad_width)\n",
        "      \n",
        "      if pad_width < 0: \n",
        "        print(mfcc.shape[1])\n",
        "        print(pad_width)\n",
        "        #print(\"error in \"+ file_path)\n",
        "      mfcc_vectors.append(mfcc)\n",
        "\n",
        "\n",
        "    np.save('/content/target_npy_files/' + label + '.npy', mfcc_vectors)\n",
        "    np.savetxt('/content/target_txt_files/' + label + '.txt', mfcc_vectors[2], delimiter=', ')\n",
        "    print(label)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['door_wood_creacks', 'chirping_birds', 'insects', 'sea_waves', 'clock_alarm', 'chainsaw', 'siren', 'frog', 'clapping', 'cow', 'rooster', 'drinking_sipping', 'clock_tick', 'can_opening', 'train', 'crickets', 'washing_machine', 'sheep', 'hen', 'thunderstorm', 'breathing', 'glass_breaking', 'water_drops', 'crackling_fire', 'engine', 'handsaw', 'brushing_teeth', 'laughing', 'car_horn', 'pouring_water', 'sneezing', 'wind', 'cat', 'airplane', 'crow', 'door_knock', 'mouse_click', 'pig', 'fireworks', 'toilet_flush', 'church_bells', 'vacuum_cleaner', 'Helicopter', 'rain', 'snoring', 'keyboard_typing', 'dog', 'crying_baby', 'coughing', 'footsteps']\n",
            "clock_alarm\n",
            "clapping\n",
            "glass_breaking\n",
            "door_knock\n",
            "fireworks\n",
            "dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdK2Ouzs9xar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "764fbef7-4e98-4caf-a43b-1869e12c9be5"
      },
      "source": [
        "split_ratio = 0.8\n",
        "random_state = 42\n",
        "\n",
        "npy_files= os.listdir('/content/target_npy_files/')\n",
        "print(npy_files)\n",
        "\n",
        "X = np.load('/content/target_npy_files/' + npy_files[0])\n",
        "y = np.zeros(X.shape[0])\n",
        "print(npy_files[0])\n",
        "print(npy_files[1])\n",
        "print(npy_files[2])\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "# # Append all of the dataset into one single array, same goes for \n",
        "for i, label in enumerate(npy_files[1:]):\n",
        "   x = np.load('/content/target_npy_files/' + label)\n",
        "  \n",
        "#     #x = np.load('/content/speech-numpy/' + label)\n",
        "\n",
        "   X = np.vstack((X, x))\n",
        "   y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
        "\n",
        "\n",
        "# return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True) \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], MFCC, 51, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], MFCC, 51, 1)\n",
        "y_train_hot = to_categorical(y_train)\n",
        "y_test_hot = to_categorical(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#mfcc_crying_baby = urban_wav2mfcc('test_single_file/crying_baby/A-5-198411-D.wav', max_pad_len=51)\n",
        "mfcc_dog = urban_wav2mfcc('test_single_file/dog/A-1-97392-A.wav', max_pad_len=51)\n",
        "mfcc_clapping = urban_wav2mfcc('test_single_file/clapping/A-1-105224-A.wav', max_pad_len=51)\n",
        "mfcc_door_knock = urban_wav2mfcc('test_single_file/door_knock/A-1-101336-A.wav', max_pad_len=51)\n",
        "mfcc_clock_alarm = urban_wav2mfcc('test_single_file/clock_alarm/A-5-210612-A.wav', max_pad_len=51)\n",
        "mfcc_glass_breaking = urban_wav2mfcc('test_single_file/glass_breaking/A-1-84705-A.wav', max_pad_len=51)\n",
        "mfcc_fireworks = urban_wav2mfcc('test_single_file/fireworks/A-1-160563-A.wav', max_pad_len=51)\n",
        "\n",
        "#crying_baby_file = mfcc_crying_baby.reshape(1, MFCC, 51, 1)\n",
        "dog_file = mfcc_dog.reshape(1, MFCC, 51, 1)\n",
        "clapping_file = mfcc_clapping.reshape(1, MFCC, 51, 1)\n",
        "door_knock_file = mfcc_door_knock.reshape(1, MFCC, 51, 1)\n",
        "clock_alarm_file = mfcc_clock_alarm.reshape(1, MFCC, 51, 1)\n",
        "glass_breaking_file = mfcc_glass_breaking.reshape(1, MFCC, 51, 1)\n",
        "fireworks_file = mfcc_fireworks.reshape(1, MFCC, 51, 1)\n",
        "\n",
        "#np.save('/content/' + 'crying_baby_single.npy', crying_baby_file)\n",
        "np.save('/content/' + 'dog_single.npy', dog_file)\n",
        "np.save('/content/' + 'clapping_single.npy', clapping_file)\n",
        "np.save('/content/' + 'door_knock_single.npy', door_knock_file )\n",
        "np.save('/content/' + 'clock_alarm_single.npy', clock_alarm_file )\n",
        "np.save('/content/' + 'glass_breaking_single.npy', glass_breaking_file)\n",
        "np.save('/content/' + 'fireworks_single.npy', fireworks_file)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['door_knock.npy', 'clapping.npy', 'glass_breaking.npy', 'clock_alarm.npy', 'fireworks.npy', 'dog.npy']\n",
            "door_knock.npy\n",
            "clapping.npy\n",
            "glass_breaking.npy\n",
            "(199, 14, 51)\n",
            "(199,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJG8-dD_BC3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bfd698c6-8aa3-4686-a5f3-a8a3c3881533"
      },
      "source": [
        "!rm checkpoint\n",
        "!rm checkpoints.data-00000-of-00001\n",
        "!rm checkpoints.index\n",
        "!rm checkpoints.meta\n",
        "\n",
        "\n",
        "speech_graph = tf.Graph()\n",
        "speech_sess = tf.Session(graph=speech_graph)\n",
        "\n",
        "keras.backend.set_session(speech_sess)\n",
        "with speech_graph.as_default():\n",
        "     \n",
        "  #build my model\n",
        "  model = build_urban_model()\n",
        "  #give me model structure\n",
        "  model.summary()\n",
        "\n",
        "  #my own optimizer\n",
        "  Amartin = optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
        "  \n",
        "  model.compile(optimizer= Amartin, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "  # run it\n",
        "  history = model.fit(X_train, y_train_hot, batch_size=200, epochs=300, verbose=1, validation_data=(X_test, y_test_hot))\n",
        "  #save my graph\n",
        "  saver = tf.train.Saver()\n",
        "  saver.save(speech_sess, 'checkpoints')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'checkpoint': No such file or directory\n",
            "rm: cannot remove 'checkpoints.data-00000-of-00001': No such file or directory\n",
            "rm: cannot remove 'checkpoints.index': No such file or directory\n",
            "rm: cannot remove 'checkpoints.meta': No such file or directory\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 7, 26, 64)         3648      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 7, 26, 64)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_1 (Separabl (None, 7, 26, 64)         4736      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 7, 26, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 26, 64)         4096      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 7, 26, 64)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 2, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 2, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 390       \n",
            "=================================================================\n",
            "Total params: 21,126\n",
            "Trainable params: 21,126\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 955 samples, validate on 239 samples\n",
            "Epoch 1/300\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "955/955 [==============================] - 3s 3ms/step - loss: 2.5358 - acc: 0.1822 - val_loss: 1.9922 - val_acc: 0.1799\n",
            "Epoch 2/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 2.1118 - acc: 0.1906 - val_loss: 1.9207 - val_acc: 0.1841\n",
            "Epoch 3/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.9620 - acc: 0.1958 - val_loss: 1.8867 - val_acc: 0.2636\n",
            "Epoch 4/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.8964 - acc: 0.2524 - val_loss: 1.8578 - val_acc: 0.3975\n",
            "Epoch 5/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.8548 - acc: 0.2691 - val_loss: 1.8265 - val_acc: 0.4561\n",
            "Epoch 6/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.8196 - acc: 0.2974 - val_loss: 1.7891 - val_acc: 0.4686\n",
            "Epoch 7/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.7885 - acc: 0.3089 - val_loss: 1.7421 - val_acc: 0.4979\n",
            "Epoch 8/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.7321 - acc: 0.3424 - val_loss: 1.6838 - val_acc: 0.4979\n",
            "Epoch 9/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.6952 - acc: 0.3613 - val_loss: 1.6189 - val_acc: 0.5356\n",
            "Epoch 10/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.6174 - acc: 0.4052 - val_loss: 1.5395 - val_acc: 0.5607\n",
            "Epoch 11/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.5649 - acc: 0.4335 - val_loss: 1.4686 - val_acc: 0.6109\n",
            "Epoch 12/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.5119 - acc: 0.4482 - val_loss: 1.4053 - val_acc: 0.6192\n",
            "Epoch 13/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.4585 - acc: 0.4901 - val_loss: 1.3604 - val_acc: 0.6067\n",
            "Epoch 14/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.3953 - acc: 0.5068 - val_loss: 1.3106 - val_acc: 0.6067\n",
            "Epoch 15/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.3660 - acc: 0.5183 - val_loss: 1.2571 - val_acc: 0.6151\n",
            "Epoch 16/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.3279 - acc: 0.5613 - val_loss: 1.2222 - val_acc: 0.6192\n",
            "Epoch 17/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.2639 - acc: 0.5686 - val_loss: 1.2023 - val_acc: 0.6192\n",
            "Epoch 18/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.2796 - acc: 0.5539 - val_loss: 1.1954 - val_acc: 0.6318\n",
            "Epoch 19/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.2201 - acc: 0.5791 - val_loss: 1.1840 - val_acc: 0.6234\n",
            "Epoch 20/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.2209 - acc: 0.6000 - val_loss: 1.1450 - val_acc: 0.6485\n",
            "Epoch 21/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.1549 - acc: 0.6084 - val_loss: 1.1223 - val_acc: 0.6527\n",
            "Epoch 22/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.1881 - acc: 0.5916 - val_loss: 1.1122 - val_acc: 0.6527\n",
            "Epoch 23/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.1449 - acc: 0.6073 - val_loss: 1.1050 - val_acc: 0.6485\n",
            "Epoch 24/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.1352 - acc: 0.6084 - val_loss: 1.0929 - val_acc: 0.6485\n",
            "Epoch 25/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.1264 - acc: 0.6126 - val_loss: 1.0788 - val_acc: 0.6653\n",
            "Epoch 26/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0654 - acc: 0.6356 - val_loss: 1.0677 - val_acc: 0.6736\n",
            "Epoch 27/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0843 - acc: 0.6377 - val_loss: 1.0588 - val_acc: 0.6653\n",
            "Epoch 28/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0993 - acc: 0.6670 - val_loss: 1.0528 - val_acc: 0.6653\n",
            "Epoch 29/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0481 - acc: 0.6461 - val_loss: 1.0505 - val_acc: 0.6611\n",
            "Epoch 30/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0882 - acc: 0.6419 - val_loss: 1.0399 - val_acc: 0.6569\n",
            "Epoch 31/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0302 - acc: 0.6785 - val_loss: 1.0128 - val_acc: 0.6904\n",
            "Epoch 32/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0292 - acc: 0.6628 - val_loss: 1.0105 - val_acc: 0.6820\n",
            "Epoch 33/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0379 - acc: 0.6586 - val_loss: 1.0244 - val_acc: 0.6485\n",
            "Epoch 34/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0217 - acc: 0.6461 - val_loss: 1.0014 - val_acc: 0.6611\n",
            "Epoch 35/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0086 - acc: 0.6482 - val_loss: 0.9873 - val_acc: 0.7071\n",
            "Epoch 36/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9804 - acc: 0.6723 - val_loss: 0.9840 - val_acc: 0.6946\n",
            "Epoch 37/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 1.0192 - acc: 0.6691 - val_loss: 0.9791 - val_acc: 0.6862\n",
            "Epoch 38/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9740 - acc: 0.6785 - val_loss: 0.9721 - val_acc: 0.6987\n",
            "Epoch 39/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9829 - acc: 0.6838 - val_loss: 0.9666 - val_acc: 0.6862\n",
            "Epoch 40/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9794 - acc: 0.6691 - val_loss: 0.9599 - val_acc: 0.7029\n",
            "Epoch 41/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9457 - acc: 0.6995 - val_loss: 0.9537 - val_acc: 0.6778\n",
            "Epoch 42/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9825 - acc: 0.6764 - val_loss: 0.9339 - val_acc: 0.7029\n",
            "Epoch 43/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9401 - acc: 0.6921 - val_loss: 0.9225 - val_acc: 0.7071\n",
            "Epoch 44/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9261 - acc: 0.6921 - val_loss: 0.9313 - val_acc: 0.6987\n",
            "Epoch 45/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9277 - acc: 0.7058 - val_loss: 0.9303 - val_acc: 0.6946\n",
            "Epoch 46/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9318 - acc: 0.6785 - val_loss: 0.9242 - val_acc: 0.6987\n",
            "Epoch 47/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9415 - acc: 0.6848 - val_loss: 0.9258 - val_acc: 0.6946\n",
            "Epoch 48/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9197 - acc: 0.6723 - val_loss: 0.9228 - val_acc: 0.6946\n",
            "Epoch 49/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9155 - acc: 0.7079 - val_loss: 0.9110 - val_acc: 0.7197\n",
            "Epoch 50/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9022 - acc: 0.7058 - val_loss: 0.9033 - val_acc: 0.7406\n",
            "Epoch 51/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9085 - acc: 0.6953 - val_loss: 0.9031 - val_acc: 0.7280\n",
            "Epoch 52/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9247 - acc: 0.6963 - val_loss: 0.9151 - val_acc: 0.7155\n",
            "Epoch 53/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8926 - acc: 0.7047 - val_loss: 0.8939 - val_acc: 0.7280\n",
            "Epoch 54/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9015 - acc: 0.7225 - val_loss: 0.8938 - val_acc: 0.7197\n",
            "Epoch 55/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.9027 - acc: 0.7047 - val_loss: 0.8877 - val_acc: 0.7238\n",
            "Epoch 56/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8907 - acc: 0.7099 - val_loss: 0.8852 - val_acc: 0.7238\n",
            "Epoch 57/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8607 - acc: 0.7204 - val_loss: 0.8903 - val_acc: 0.7238\n",
            "Epoch 58/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8837 - acc: 0.7120 - val_loss: 0.8770 - val_acc: 0.7280\n",
            "Epoch 59/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8684 - acc: 0.7089 - val_loss: 0.8694 - val_acc: 0.7280\n",
            "Epoch 60/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8365 - acc: 0.7277 - val_loss: 0.8710 - val_acc: 0.7197\n",
            "Epoch 61/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8528 - acc: 0.7110 - val_loss: 0.8661 - val_acc: 0.7238\n",
            "Epoch 62/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8686 - acc: 0.7110 - val_loss: 0.8620 - val_acc: 0.7364\n",
            "Epoch 63/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8376 - acc: 0.7267 - val_loss: 0.8693 - val_acc: 0.7406\n",
            "Epoch 64/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8664 - acc: 0.7298 - val_loss: 0.8524 - val_acc: 0.7280\n",
            "Epoch 65/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8440 - acc: 0.7194 - val_loss: 0.8535 - val_acc: 0.7364\n",
            "Epoch 66/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8411 - acc: 0.7330 - val_loss: 0.8538 - val_acc: 0.7406\n",
            "Epoch 67/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8298 - acc: 0.7382 - val_loss: 0.8446 - val_acc: 0.7490\n",
            "Epoch 68/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8452 - acc: 0.6932 - val_loss: 0.8381 - val_acc: 0.7364\n",
            "Epoch 69/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8227 - acc: 0.7246 - val_loss: 0.8476 - val_acc: 0.7406\n",
            "Epoch 70/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8461 - acc: 0.7487 - val_loss: 0.8447 - val_acc: 0.7406\n",
            "Epoch 71/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8167 - acc: 0.7414 - val_loss: 0.8302 - val_acc: 0.7615\n",
            "Epoch 72/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8083 - acc: 0.7476 - val_loss: 0.8242 - val_acc: 0.7573\n",
            "Epoch 73/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8239 - acc: 0.7393 - val_loss: 0.8170 - val_acc: 0.7531\n",
            "Epoch 74/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7959 - acc: 0.7592 - val_loss: 0.8057 - val_acc: 0.7615\n",
            "Epoch 75/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8143 - acc: 0.7403 - val_loss: 0.8060 - val_acc: 0.7615\n",
            "Epoch 76/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8498 - acc: 0.7330 - val_loss: 0.8238 - val_acc: 0.7490\n",
            "Epoch 77/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8155 - acc: 0.7382 - val_loss: 0.8187 - val_acc: 0.7573\n",
            "Epoch 78/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7873 - acc: 0.7309 - val_loss: 0.8216 - val_acc: 0.7406\n",
            "Epoch 79/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7979 - acc: 0.7497 - val_loss: 0.8134 - val_acc: 0.7573\n",
            "Epoch 80/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.8035 - acc: 0.7455 - val_loss: 0.8042 - val_acc: 0.7657\n",
            "Epoch 81/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7935 - acc: 0.7403 - val_loss: 0.8014 - val_acc: 0.7490\n",
            "Epoch 82/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7609 - acc: 0.7466 - val_loss: 0.7991 - val_acc: 0.7490\n",
            "Epoch 83/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7755 - acc: 0.7539 - val_loss: 0.7940 - val_acc: 0.7573\n",
            "Epoch 84/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7809 - acc: 0.7455 - val_loss: 0.7960 - val_acc: 0.7406\n",
            "Epoch 85/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7912 - acc: 0.7602 - val_loss: 0.7943 - val_acc: 0.7531\n",
            "Epoch 86/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7984 - acc: 0.7351 - val_loss: 0.7889 - val_acc: 0.7573\n",
            "Epoch 87/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7839 - acc: 0.7508 - val_loss: 0.7865 - val_acc: 0.7573\n",
            "Epoch 88/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7667 - acc: 0.7717 - val_loss: 0.7831 - val_acc: 0.7657\n",
            "Epoch 89/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7527 - acc: 0.7602 - val_loss: 0.7826 - val_acc: 0.7699\n",
            "Epoch 90/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7705 - acc: 0.7497 - val_loss: 0.7836 - val_acc: 0.7573\n",
            "Epoch 91/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7542 - acc: 0.7602 - val_loss: 0.7828 - val_acc: 0.7657\n",
            "Epoch 92/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7536 - acc: 0.7602 - val_loss: 0.7742 - val_acc: 0.7699\n",
            "Epoch 93/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7686 - acc: 0.7623 - val_loss: 0.7765 - val_acc: 0.7573\n",
            "Epoch 94/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7588 - acc: 0.7518 - val_loss: 0.7828 - val_acc: 0.7615\n",
            "Epoch 95/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7909 - acc: 0.7393 - val_loss: 0.7708 - val_acc: 0.7782\n",
            "Epoch 96/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7559 - acc: 0.7707 - val_loss: 0.7707 - val_acc: 0.7699\n",
            "Epoch 97/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7503 - acc: 0.7644 - val_loss: 0.7673 - val_acc: 0.7657\n",
            "Epoch 98/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7858 - acc: 0.7487 - val_loss: 0.7778 - val_acc: 0.7699\n",
            "Epoch 99/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7383 - acc: 0.7581 - val_loss: 0.7886 - val_acc: 0.7657\n",
            "Epoch 100/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7664 - acc: 0.7508 - val_loss: 0.7585 - val_acc: 0.7615\n",
            "Epoch 101/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7781 - acc: 0.7550 - val_loss: 0.7531 - val_acc: 0.7782\n",
            "Epoch 102/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7553 - acc: 0.7675 - val_loss: 0.7607 - val_acc: 0.7699\n",
            "Epoch 103/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7457 - acc: 0.7634 - val_loss: 0.7639 - val_acc: 0.7699\n",
            "Epoch 104/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7130 - acc: 0.7707 - val_loss: 0.7515 - val_acc: 0.7741\n",
            "Epoch 105/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7494 - acc: 0.7654 - val_loss: 0.7535 - val_acc: 0.7699\n",
            "Epoch 106/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7274 - acc: 0.7675 - val_loss: 0.7531 - val_acc: 0.7741\n",
            "Epoch 107/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7072 - acc: 0.7864 - val_loss: 0.7429 - val_acc: 0.7782\n",
            "Epoch 108/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7378 - acc: 0.7696 - val_loss: 0.7485 - val_acc: 0.7741\n",
            "Epoch 109/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7314 - acc: 0.7665 - val_loss: 0.7557 - val_acc: 0.7699\n",
            "Epoch 110/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7410 - acc: 0.7623 - val_loss: 0.7641 - val_acc: 0.7782\n",
            "Epoch 111/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7242 - acc: 0.7696 - val_loss: 0.7560 - val_acc: 0.7573\n",
            "Epoch 112/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7178 - acc: 0.7791 - val_loss: 0.7535 - val_acc: 0.7657\n",
            "Epoch 113/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7218 - acc: 0.7675 - val_loss: 0.7505 - val_acc: 0.7741\n",
            "Epoch 114/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7370 - acc: 0.7696 - val_loss: 0.7381 - val_acc: 0.7699\n",
            "Epoch 115/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7412 - acc: 0.7686 - val_loss: 0.7442 - val_acc: 0.7950\n",
            "Epoch 116/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7382 - acc: 0.7592 - val_loss: 0.7429 - val_acc: 0.7908\n",
            "Epoch 117/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7215 - acc: 0.7665 - val_loss: 0.7335 - val_acc: 0.7782\n",
            "Epoch 118/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7252 - acc: 0.7644 - val_loss: 0.7364 - val_acc: 0.7699\n",
            "Epoch 119/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7313 - acc: 0.7675 - val_loss: 0.7273 - val_acc: 0.7782\n",
            "Epoch 120/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7009 - acc: 0.7801 - val_loss: 0.7286 - val_acc: 0.7908\n",
            "Epoch 121/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7244 - acc: 0.7634 - val_loss: 0.7345 - val_acc: 0.7866\n",
            "Epoch 122/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6954 - acc: 0.7812 - val_loss: 0.7241 - val_acc: 0.7866\n",
            "Epoch 123/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6760 - acc: 0.7864 - val_loss: 0.7290 - val_acc: 0.7866\n",
            "Epoch 124/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7043 - acc: 0.7780 - val_loss: 0.7277 - val_acc: 0.7824\n",
            "Epoch 125/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.7042 - acc: 0.7770 - val_loss: 0.7360 - val_acc: 0.7866\n",
            "Epoch 126/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6986 - acc: 0.7791 - val_loss: 0.7346 - val_acc: 0.7824\n",
            "Epoch 127/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6902 - acc: 0.7728 - val_loss: 0.7275 - val_acc: 0.7782\n",
            "Epoch 128/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6859 - acc: 0.7895 - val_loss: 0.7161 - val_acc: 0.7908\n",
            "Epoch 129/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6950 - acc: 0.7623 - val_loss: 0.7248 - val_acc: 0.7824\n",
            "Epoch 130/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6990 - acc: 0.7696 - val_loss: 0.7242 - val_acc: 0.7866\n",
            "Epoch 131/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6796 - acc: 0.7885 - val_loss: 0.7225 - val_acc: 0.7908\n",
            "Epoch 132/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6696 - acc: 0.7812 - val_loss: 0.7308 - val_acc: 0.7908\n",
            "Epoch 133/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6985 - acc: 0.7801 - val_loss: 0.7164 - val_acc: 0.7992\n",
            "Epoch 134/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6938 - acc: 0.7759 - val_loss: 0.7068 - val_acc: 0.7908\n",
            "Epoch 135/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6660 - acc: 0.8052 - val_loss: 0.7056 - val_acc: 0.7950\n",
            "Epoch 136/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6919 - acc: 0.7832 - val_loss: 0.7140 - val_acc: 0.8117\n",
            "Epoch 137/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6852 - acc: 0.7728 - val_loss: 0.6966 - val_acc: 0.7992\n",
            "Epoch 138/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6814 - acc: 0.7864 - val_loss: 0.7028 - val_acc: 0.7950\n",
            "Epoch 139/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6765 - acc: 0.7874 - val_loss: 0.7044 - val_acc: 0.7866\n",
            "Epoch 140/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6809 - acc: 0.7843 - val_loss: 0.7104 - val_acc: 0.7992\n",
            "Epoch 141/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6564 - acc: 0.7895 - val_loss: 0.6995 - val_acc: 0.8033\n",
            "Epoch 142/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6683 - acc: 0.7822 - val_loss: 0.7010 - val_acc: 0.7950\n",
            "Epoch 143/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6543 - acc: 0.8000 - val_loss: 0.7002 - val_acc: 0.7992\n",
            "Epoch 144/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6938 - acc: 0.7801 - val_loss: 0.6959 - val_acc: 0.7950\n",
            "Epoch 145/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6745 - acc: 0.7791 - val_loss: 0.6987 - val_acc: 0.8075\n",
            "Epoch 146/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6620 - acc: 0.7843 - val_loss: 0.6969 - val_acc: 0.7908\n",
            "Epoch 147/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6714 - acc: 0.7832 - val_loss: 0.7005 - val_acc: 0.7992\n",
            "Epoch 148/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6669 - acc: 0.7864 - val_loss: 0.6949 - val_acc: 0.8075\n",
            "Epoch 149/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6567 - acc: 0.7937 - val_loss: 0.6961 - val_acc: 0.8075\n",
            "Epoch 150/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6598 - acc: 0.7979 - val_loss: 0.6879 - val_acc: 0.8159\n",
            "Epoch 151/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6643 - acc: 0.7895 - val_loss: 0.6864 - val_acc: 0.8159\n",
            "Epoch 152/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6382 - acc: 0.8063 - val_loss: 0.6957 - val_acc: 0.7992\n",
            "Epoch 153/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6555 - acc: 0.8073 - val_loss: 0.7088 - val_acc: 0.7908\n",
            "Epoch 154/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6452 - acc: 0.7948 - val_loss: 0.6953 - val_acc: 0.8033\n",
            "Epoch 155/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6514 - acc: 0.7853 - val_loss: 0.6901 - val_acc: 0.8033\n",
            "Epoch 156/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6460 - acc: 0.7948 - val_loss: 0.6769 - val_acc: 0.8159\n",
            "Epoch 157/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6686 - acc: 0.8000 - val_loss: 0.6772 - val_acc: 0.8159\n",
            "Epoch 158/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6531 - acc: 0.7979 - val_loss: 0.6854 - val_acc: 0.8117\n",
            "Epoch 159/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6396 - acc: 0.8021 - val_loss: 0.6794 - val_acc: 0.7866\n",
            "Epoch 160/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6578 - acc: 0.7916 - val_loss: 0.6845 - val_acc: 0.8075\n",
            "Epoch 161/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6306 - acc: 0.7990 - val_loss: 0.6809 - val_acc: 0.8159\n",
            "Epoch 162/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6546 - acc: 0.7885 - val_loss: 0.6797 - val_acc: 0.8075\n",
            "Epoch 163/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6395 - acc: 0.7927 - val_loss: 0.6828 - val_acc: 0.8075\n",
            "Epoch 164/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6198 - acc: 0.7990 - val_loss: 0.6897 - val_acc: 0.7992\n",
            "Epoch 165/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6475 - acc: 0.7916 - val_loss: 0.6820 - val_acc: 0.7992\n",
            "Epoch 166/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6537 - acc: 0.7916 - val_loss: 0.6843 - val_acc: 0.8075\n",
            "Epoch 167/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6353 - acc: 0.8000 - val_loss: 0.6764 - val_acc: 0.8033\n",
            "Epoch 168/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6559 - acc: 0.7864 - val_loss: 0.6695 - val_acc: 0.8117\n",
            "Epoch 169/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6347 - acc: 0.8010 - val_loss: 0.6724 - val_acc: 0.8159\n",
            "Epoch 170/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6110 - acc: 0.7948 - val_loss: 0.6702 - val_acc: 0.7992\n",
            "Epoch 171/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6308 - acc: 0.7937 - val_loss: 0.6701 - val_acc: 0.7908\n",
            "Epoch 172/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6347 - acc: 0.7990 - val_loss: 0.6754 - val_acc: 0.7992\n",
            "Epoch 173/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6355 - acc: 0.7822 - val_loss: 0.6686 - val_acc: 0.8117\n",
            "Epoch 174/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6150 - acc: 0.8094 - val_loss: 0.6606 - val_acc: 0.8117\n",
            "Epoch 175/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6422 - acc: 0.8010 - val_loss: 0.6621 - val_acc: 0.7992\n",
            "Epoch 176/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6397 - acc: 0.8010 - val_loss: 0.6671 - val_acc: 0.8075\n",
            "Epoch 177/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6103 - acc: 0.8157 - val_loss: 0.6798 - val_acc: 0.8075\n",
            "Epoch 178/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6405 - acc: 0.7937 - val_loss: 0.6698 - val_acc: 0.8033\n",
            "Epoch 179/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6154 - acc: 0.8031 - val_loss: 0.6668 - val_acc: 0.8159\n",
            "Epoch 180/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6339 - acc: 0.8031 - val_loss: 0.6736 - val_acc: 0.7992\n",
            "Epoch 181/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6024 - acc: 0.8042 - val_loss: 0.6692 - val_acc: 0.8033\n",
            "Epoch 182/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6092 - acc: 0.8115 - val_loss: 0.6624 - val_acc: 0.8033\n",
            "Epoch 183/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6238 - acc: 0.7979 - val_loss: 0.6630 - val_acc: 0.7992\n",
            "Epoch 184/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6047 - acc: 0.8021 - val_loss: 0.6718 - val_acc: 0.8033\n",
            "Epoch 185/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6352 - acc: 0.7895 - val_loss: 0.6647 - val_acc: 0.7908\n",
            "Epoch 186/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6195 - acc: 0.8147 - val_loss: 0.6658 - val_acc: 0.7950\n",
            "Epoch 187/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6066 - acc: 0.8073 - val_loss: 0.6736 - val_acc: 0.7950\n",
            "Epoch 188/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6204 - acc: 0.8042 - val_loss: 0.6727 - val_acc: 0.7950\n",
            "Epoch 189/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6129 - acc: 0.7906 - val_loss: 0.6599 - val_acc: 0.8075\n",
            "Epoch 190/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6053 - acc: 0.8052 - val_loss: 0.6565 - val_acc: 0.8033\n",
            "Epoch 191/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6187 - acc: 0.8010 - val_loss: 0.6666 - val_acc: 0.7908\n",
            "Epoch 192/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6288 - acc: 0.7990 - val_loss: 0.6760 - val_acc: 0.7908\n",
            "Epoch 193/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6074 - acc: 0.8063 - val_loss: 0.6648 - val_acc: 0.7992\n",
            "Epoch 194/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6081 - acc: 0.7948 - val_loss: 0.6619 - val_acc: 0.7992\n",
            "Epoch 195/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6122 - acc: 0.8031 - val_loss: 0.6628 - val_acc: 0.8075\n",
            "Epoch 196/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6003 - acc: 0.8157 - val_loss: 0.6614 - val_acc: 0.8117\n",
            "Epoch 197/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6188 - acc: 0.8094 - val_loss: 0.6594 - val_acc: 0.7950\n",
            "Epoch 198/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6135 - acc: 0.8188 - val_loss: 0.6606 - val_acc: 0.8033\n",
            "Epoch 199/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6104 - acc: 0.8000 - val_loss: 0.6665 - val_acc: 0.8033\n",
            "Epoch 200/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6007 - acc: 0.8157 - val_loss: 0.6518 - val_acc: 0.8075\n",
            "Epoch 201/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5985 - acc: 0.8147 - val_loss: 0.6486 - val_acc: 0.8117\n",
            "Epoch 202/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5984 - acc: 0.8126 - val_loss: 0.6528 - val_acc: 0.8117\n",
            "Epoch 203/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6040 - acc: 0.7990 - val_loss: 0.6540 - val_acc: 0.8075\n",
            "Epoch 204/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5942 - acc: 0.8199 - val_loss: 0.6442 - val_acc: 0.7992\n",
            "Epoch 205/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5868 - acc: 0.8094 - val_loss: 0.6429 - val_acc: 0.8117\n",
            "Epoch 206/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6030 - acc: 0.8000 - val_loss: 0.6506 - val_acc: 0.8117\n",
            "Epoch 207/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5845 - acc: 0.8073 - val_loss: 0.6622 - val_acc: 0.8075\n",
            "Epoch 208/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6035 - acc: 0.8021 - val_loss: 0.6559 - val_acc: 0.8033\n",
            "Epoch 209/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5791 - acc: 0.8241 - val_loss: 0.6545 - val_acc: 0.8117\n",
            "Epoch 210/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5930 - acc: 0.8136 - val_loss: 0.6560 - val_acc: 0.8033\n",
            "Epoch 211/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6089 - acc: 0.8073 - val_loss: 0.6505 - val_acc: 0.7992\n",
            "Epoch 212/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5940 - acc: 0.8010 - val_loss: 0.6378 - val_acc: 0.8075\n",
            "Epoch 213/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5925 - acc: 0.8073 - val_loss: 0.6346 - val_acc: 0.8075\n",
            "Epoch 214/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5856 - acc: 0.8209 - val_loss: 0.6522 - val_acc: 0.7992\n",
            "Epoch 215/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5734 - acc: 0.8126 - val_loss: 0.6418 - val_acc: 0.7992\n",
            "Epoch 216/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6043 - acc: 0.8147 - val_loss: 0.6360 - val_acc: 0.8033\n",
            "Epoch 217/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5875 - acc: 0.8115 - val_loss: 0.6408 - val_acc: 0.8033\n",
            "Epoch 218/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5859 - acc: 0.8147 - val_loss: 0.6553 - val_acc: 0.8033\n",
            "Epoch 219/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6005 - acc: 0.8094 - val_loss: 0.6397 - val_acc: 0.8117\n",
            "Epoch 220/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5814 - acc: 0.8126 - val_loss: 0.6362 - val_acc: 0.8033\n",
            "Epoch 221/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5854 - acc: 0.8168 - val_loss: 0.6349 - val_acc: 0.8159\n",
            "Epoch 222/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.6003 - acc: 0.8157 - val_loss: 0.6420 - val_acc: 0.8075\n",
            "Epoch 223/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5787 - acc: 0.8220 - val_loss: 0.6397 - val_acc: 0.8033\n",
            "Epoch 224/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5689 - acc: 0.8325 - val_loss: 0.6443 - val_acc: 0.8075\n",
            "Epoch 225/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5752 - acc: 0.8168 - val_loss: 0.6392 - val_acc: 0.8159\n",
            "Epoch 226/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5728 - acc: 0.8188 - val_loss: 0.6323 - val_acc: 0.8201\n",
            "Epoch 227/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5621 - acc: 0.8346 - val_loss: 0.6279 - val_acc: 0.8159\n",
            "Epoch 228/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5652 - acc: 0.8126 - val_loss: 0.6294 - val_acc: 0.8075\n",
            "Epoch 229/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5622 - acc: 0.8105 - val_loss: 0.6319 - val_acc: 0.8075\n",
            "Epoch 230/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5657 - acc: 0.8188 - val_loss: 0.6439 - val_acc: 0.8033\n",
            "Epoch 231/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5642 - acc: 0.8199 - val_loss: 0.6357 - val_acc: 0.8075\n",
            "Epoch 232/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5500 - acc: 0.8230 - val_loss: 0.6331 - val_acc: 0.8159\n",
            "Epoch 233/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5752 - acc: 0.8084 - val_loss: 0.6333 - val_acc: 0.8075\n",
            "Epoch 234/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5588 - acc: 0.8168 - val_loss: 0.6349 - val_acc: 0.8117\n",
            "Epoch 235/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5571 - acc: 0.8188 - val_loss: 0.6322 - val_acc: 0.8117\n",
            "Epoch 236/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5699 - acc: 0.8230 - val_loss: 0.6262 - val_acc: 0.8033\n",
            "Epoch 237/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5574 - acc: 0.8199 - val_loss: 0.6250 - val_acc: 0.8033\n",
            "Epoch 238/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5685 - acc: 0.8178 - val_loss: 0.6287 - val_acc: 0.7950\n",
            "Epoch 239/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5739 - acc: 0.8094 - val_loss: 0.6279 - val_acc: 0.8117\n",
            "Epoch 240/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5733 - acc: 0.8084 - val_loss: 0.6216 - val_acc: 0.8201\n",
            "Epoch 241/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5813 - acc: 0.8084 - val_loss: 0.6143 - val_acc: 0.8201\n",
            "Epoch 242/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5683 - acc: 0.8168 - val_loss: 0.6454 - val_acc: 0.8201\n",
            "Epoch 243/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5402 - acc: 0.8346 - val_loss: 0.6267 - val_acc: 0.8033\n",
            "Epoch 244/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5733 - acc: 0.8094 - val_loss: 0.6247 - val_acc: 0.8033\n",
            "Epoch 245/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5744 - acc: 0.8105 - val_loss: 0.6193 - val_acc: 0.8159\n",
            "Epoch 246/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5778 - acc: 0.8115 - val_loss: 0.6140 - val_acc: 0.8159\n",
            "Epoch 247/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5689 - acc: 0.8168 - val_loss: 0.6143 - val_acc: 0.8075\n",
            "Epoch 248/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5482 - acc: 0.8147 - val_loss: 0.6113 - val_acc: 0.8159\n",
            "Epoch 249/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5462 - acc: 0.8209 - val_loss: 0.6151 - val_acc: 0.8075\n",
            "Epoch 250/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5586 - acc: 0.8188 - val_loss: 0.6146 - val_acc: 0.8075\n",
            "Epoch 251/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5749 - acc: 0.8115 - val_loss: 0.6156 - val_acc: 0.8075\n",
            "Epoch 252/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5488 - acc: 0.8262 - val_loss: 0.6231 - val_acc: 0.8117\n",
            "Epoch 253/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5595 - acc: 0.8209 - val_loss: 0.6212 - val_acc: 0.8117\n",
            "Epoch 254/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5598 - acc: 0.8094 - val_loss: 0.6191 - val_acc: 0.8033\n",
            "Epoch 255/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5537 - acc: 0.8178 - val_loss: 0.6127 - val_acc: 0.8033\n",
            "Epoch 256/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5451 - acc: 0.8241 - val_loss: 0.6215 - val_acc: 0.8033\n",
            "Epoch 257/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5460 - acc: 0.8272 - val_loss: 0.6234 - val_acc: 0.8075\n",
            "Epoch 258/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5598 - acc: 0.8262 - val_loss: 0.6157 - val_acc: 0.8117\n",
            "Epoch 259/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5503 - acc: 0.8230 - val_loss: 0.6226 - val_acc: 0.8117\n",
            "Epoch 260/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5555 - acc: 0.8147 - val_loss: 0.6180 - val_acc: 0.8243\n",
            "Epoch 261/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5729 - acc: 0.8335 - val_loss: 0.6164 - val_acc: 0.8033\n",
            "Epoch 262/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5536 - acc: 0.8262 - val_loss: 0.6198 - val_acc: 0.8075\n",
            "Epoch 263/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5396 - acc: 0.8283 - val_loss: 0.6172 - val_acc: 0.8117\n",
            "Epoch 264/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5458 - acc: 0.8199 - val_loss: 0.6045 - val_acc: 0.8117\n",
            "Epoch 265/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5343 - acc: 0.8199 - val_loss: 0.6144 - val_acc: 0.8117\n",
            "Epoch 266/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5349 - acc: 0.8325 - val_loss: 0.6192 - val_acc: 0.8033\n",
            "Epoch 267/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5414 - acc: 0.8178 - val_loss: 0.6099 - val_acc: 0.8159\n",
            "Epoch 268/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5275 - acc: 0.8188 - val_loss: 0.6041 - val_acc: 0.8159\n",
            "Epoch 269/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5454 - acc: 0.8241 - val_loss: 0.6033 - val_acc: 0.8033\n",
            "Epoch 270/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5189 - acc: 0.8356 - val_loss: 0.6068 - val_acc: 0.8117\n",
            "Epoch 271/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5392 - acc: 0.8199 - val_loss: 0.6028 - val_acc: 0.8159\n",
            "Epoch 272/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5166 - acc: 0.8398 - val_loss: 0.6030 - val_acc: 0.8159\n",
            "Epoch 273/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5254 - acc: 0.8220 - val_loss: 0.6060 - val_acc: 0.8117\n",
            "Epoch 274/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5294 - acc: 0.8325 - val_loss: 0.6081 - val_acc: 0.8075\n",
            "Epoch 275/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5262 - acc: 0.8230 - val_loss: 0.6045 - val_acc: 0.8201\n",
            "Epoch 276/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5320 - acc: 0.8293 - val_loss: 0.6079 - val_acc: 0.8201\n",
            "Epoch 277/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5254 - acc: 0.8241 - val_loss: 0.6092 - val_acc: 0.8159\n",
            "Epoch 278/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5283 - acc: 0.8304 - val_loss: 0.6138 - val_acc: 0.8117\n",
            "Epoch 279/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5558 - acc: 0.8136 - val_loss: 0.6084 - val_acc: 0.8201\n",
            "Epoch 280/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5501 - acc: 0.8168 - val_loss: 0.6084 - val_acc: 0.8159\n",
            "Epoch 281/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5419 - acc: 0.8241 - val_loss: 0.6253 - val_acc: 0.8033\n",
            "Epoch 282/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5393 - acc: 0.8199 - val_loss: 0.6153 - val_acc: 0.8117\n",
            "Epoch 283/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5439 - acc: 0.8241 - val_loss: 0.6025 - val_acc: 0.8117\n",
            "Epoch 284/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5298 - acc: 0.8304 - val_loss: 0.6029 - val_acc: 0.8159\n",
            "Epoch 285/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5332 - acc: 0.8314 - val_loss: 0.5930 - val_acc: 0.8075\n",
            "Epoch 286/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5442 - acc: 0.8178 - val_loss: 0.5913 - val_acc: 0.8117\n",
            "Epoch 287/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5275 - acc: 0.8335 - val_loss: 0.6043 - val_acc: 0.8117\n",
            "Epoch 288/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5144 - acc: 0.8262 - val_loss: 0.5935 - val_acc: 0.8117\n",
            "Epoch 289/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5372 - acc: 0.8251 - val_loss: 0.5948 - val_acc: 0.8075\n",
            "Epoch 290/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5376 - acc: 0.8283 - val_loss: 0.5910 - val_acc: 0.8201\n",
            "Epoch 291/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5433 - acc: 0.8178 - val_loss: 0.5967 - val_acc: 0.8326\n",
            "Epoch 292/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5416 - acc: 0.8168 - val_loss: 0.6083 - val_acc: 0.8117\n",
            "Epoch 293/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5430 - acc: 0.8408 - val_loss: 0.6089 - val_acc: 0.8117\n",
            "Epoch 294/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5516 - acc: 0.8188 - val_loss: 0.6015 - val_acc: 0.8117\n",
            "Epoch 295/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5322 - acc: 0.8356 - val_loss: 0.6052 - val_acc: 0.8033\n",
            "Epoch 296/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5375 - acc: 0.8304 - val_loss: 0.5931 - val_acc: 0.8159\n",
            "Epoch 297/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5337 - acc: 0.8272 - val_loss: 0.5984 - val_acc: 0.8285\n",
            "Epoch 298/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5034 - acc: 0.8304 - val_loss: 0.5976 - val_acc: 0.8075\n",
            "Epoch 299/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5550 - acc: 0.8147 - val_loss: 0.6130 - val_acc: 0.8075\n",
            "Epoch 300/300\n",
            "955/955 [==============================] - 1s 1ms/step - loss: 0.5162 - acc: 0.8283 - val_loss: 0.5989 - val_acc: 0.8159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnEpd1YcC1yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "8de41e28-a2ec-4746-d9ff-59f628ec0d4e"
      },
      "source": [
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs,val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkIK7iEgOUnAXEclB/x/yCgBdyWpd5gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEICAYAAABYoZ8gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAzATKmuC-bg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "e4336104-6260-423e-df41-faf9ca74ab86"
      },
      "source": [
        "eval_speech_graph = tf.Graph()\n",
        "eval_speech_sess = tf.Session(graph=eval_speech_graph)\n",
        "\n",
        "keras.backend.set_session(eval_speech_sess)\n",
        "with eval_speech_graph.as_default():\n",
        "    keras.backend.set_learning_phase(0)\n",
        "    eval_model = build_urban_model()\n",
        "\n",
        "    #For quantization aware training only\n",
        "    #tf.contrib.quantize.create_eval_graph(input_graph=eval_speech_graph)\n",
        "    eval_speech_graph_def = eval_speech_graph.as_graph_def()\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(eval_speech_sess, 'checkpoints')\n",
        "\n",
        "    frozen_graph_def = tf.graph_util.convert_variables_to_constants( eval_speech_sess, eval_speech_graph_def, \n",
        "                                                                    [eval_model.output.op.name] )\n",
        "\n",
        "    with open('frozen_urban_model.pb', 'wb') as f:\n",
        "      f.write(frozen_graph_def.SerializeToString())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints\n",
            "WARNING:tensorflow:From <ipython-input-12-0f3293bd894d>:16: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 10 variables.\n",
            "INFO:tensorflow:Converted 10 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMQ7P0SfDAPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_frozen_graph( \"frozen_urban_model.pb\", [\"conv2d_1_input\"], [\"dense_2/Softmax\"])\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58yDd2ZaDJfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tflite_models_dir = pathlib.Path(\"/tmp/ESC50-split-models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9ss0LxXDTaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dog_uint8 = tf.cast(dog_file, tf.uint8)\n",
        "clapping_uint8 = tf.cast(clapping_file, tf.uint8)\n",
        "door_knock_uint8 = tf.cast(door_knock_file, tf.uint8)\n",
        "clock_alarm_uint8 = tf.cast(clock_alarm_file, tf.uint8)\n",
        "glass_breaking_uint8 = tf.cast(glass_breaking_file, tf.uint8)\n",
        "fireworks_uint8 = tf.cast(fireworks_file, tf.uint8)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdlDx4RcDzAk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('/content/' + 'dog_single_uint8.npy', dog_uint8)\n",
        "np.save('/content/' + 'clapping_single_uint8.npy', clapping_uint8)\n",
        "np.save('/content/' + 'door_knock_single_uint8.npy', door_knock_uint8)\n",
        "np.save('/content/' + 'clock_alarm_single_uint8.npy', clock_alarm_uint8 )\n",
        "np.save('/content/' + 'glass_breaking_single_uint8.npy', glass_breaking_uint8)\n",
        "np.save('/content/' + 'fireworks_single_uint8.npy', fireworks_uint8 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuGzRYavES2u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9332a422-1461-4ae9-d4f8-320ae3981e66"
      },
      "source": [
        "tflite_model_file = tflite_models_dir/\"esc50-split-model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVeRww2LEc4h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1d6bb369-ebbf-4667-b6c4-daa2c1009236"
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 88K\n",
            "-rw-r--r-- 1 root root 86K Oct 17 02:50 esc50-split-model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNWiiVE2EfT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimize for size\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-2qnmQfEiHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create quantized values with an accurate dynamic range of activations, \n",
        "# for that need to provide a representative dataset\n",
        "\n",
        "#sounds = tf.cast(X_train, tf.float32)/255.0\n",
        "sounds = tf.cast(X_train, tf.float32)\n",
        "urban_ds = tf.data.Dataset.from_tensor_slices((sounds)).batch(1)\n",
        "def representative_data_gen():\n",
        "  for input_value in urban_ds.take(100):\n",
        "    yield [input_value]\n",
        "    \n",
        "converter.representative_dataset = representative_data_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFUqmRgMEmCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "01903469-e6ab-4766-940f-7bca72045e5a"
      },
      "source": [
        "tflite_quant_model = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"esc50-split-model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_quant_model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppMZclqFEuK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a962a5ee-b21a-4e86-87ef-f3e621caa7e0"
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 120K\n",
            "-rw-r--r-- 1 root root 31K Oct 17 02:50 esc50-split-model_quant.tflite\n",
            "-rw-r--r-- 1 root root 86K Oct 17 02:50 esc50-split-model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhLSS-0YEvNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cccf3013-b249-4600-e437-175569769881"
      },
      "source": [
        "# The converted model needs to be fully quantized. That means all ops need to be \n",
        "# quantized, no floats left. The input and outputs need to be integers too.\n",
        "\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"esc50-split-model_quant_io.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9RbRpDPE2tc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "fc41b2bf-cf31-4a3c-b790-3b9a187acd67"
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 152K\n",
            "-rw-r--r-- 1 root root 31K Oct 17 02:50 esc50-split-model_quant_io.tflite\n",
            "-rw-r--r-- 1 root root 31K Oct 17 02:50 esc50-split-model_quant.tflite\n",
            "-rw-r--r-- 1 root root 86K Oct 17 02:50 esc50-split-model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyyVI07xE52n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d80428db-4136-4fe2-9b89-dda651f3a35a"
      },
      "source": [
        "# Load data for float model\n",
        "#sounds = tf.cast(X_test, tf.float32)/255.0\n",
        "sounds = tf.cast(X_test, tf.float32)\n",
        "urban_ds = tf.data.Dataset.from_tensor_slices(sounds).batch(1)\n",
        "\n",
        "# Load data for quantized model\n",
        "sounds_uint8 = tf.cast(X_test, tf.uint8)\n",
        "urban_ds_uint8 = tf.data.Dataset.from_tensor_slices(sounds_uint8).batch(1)\n",
        "print(urban_ds)\n",
        "print(urban_ds_uint8)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: (?, 14, 51, 1), types: tf.float32>\n",
            "<DatasetV1Adapter shapes: (?, 14, 51, 1), types: tf.uint8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4f13wGME7e3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Grab files to test\n",
        "mfcc_dog = urban_wav2mfcc('test_single_file/dog/A-1-97392-A.wav', max_pad_len=51)\n",
        "mfcc_clapping = urban_wav2mfcc('test_single_file/clapping/A-1-105224-A.wav', max_pad_len=51)\n",
        "mfcc_door_knock = urban_wav2mfcc('test_single_file/door_knock/A-1-101336-A.wav', max_pad_len=51)\n",
        "mfcc_clock_alarm = urban_wav2mfcc('test_single_file/clock_alarm/A-5-210612-A.wav', max_pad_len=51)\n",
        "mfcc_glass_breaking = urban_wav2mfcc('test_single_file/glass_breaking/A-1-84705-A.wav', max_pad_len=51)\n",
        "mfcc_fireworks = urban_wav2mfcc('test_single_file/fireworks/A-1-160563-A.wav', max_pad_len=51)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKHktbjMFIvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2D to 1D tensor\n",
        "mfcc_dog_flat = mfcc_dog.flatten()\n",
        "mfcc_clapping_flat = mfcc_clapping.flatten()\n",
        "mfcc_door_knock_flat = mfcc_door_knock.flatten()\n",
        "mfcc_clock_alarm_flat = mfcc_clock_alarm.flatten()\n",
        "mfcc_glass_breaking_flat = mfcc_glass_breaking.flatten()\n",
        "mfcc_fireworks_flat = mfcc_fireworks.flatten()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA-L6fpxFmZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Quantize \n",
        "dog_uint8 = tf.cast(mfcc_dog_flat, tf.uint8)\n",
        "clapping_uint8 = tf.cast(mfcc_clapping_flat, tf.uint8)\n",
        "door_knock_uint8 = tf.cast(mfcc_door_knock_flat, tf.uint8)\n",
        "clock_alarm_uint8 = tf.cast(mfcc_clock_alarm_flat , tf.uint8)\n",
        "glass_breaking_uint8 = tf.cast(mfcc_glass_breaking_flat, tf.uint8)\n",
        "fireworks_uint8 = tf.cast(mfcc_fireworks_flat, tf.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTMoIqgbGY3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savetxt('dog_uint8.txt', [dog_uint8], fmt='%i, ', header='{', footer='}', comments='')\n",
        "np.savetxt('clapping_uint8.txt', [clapping_uint8], fmt='%i, ', header='{', footer='}', comments='')\n",
        "np.savetxt('door_knock_uint8.txt', [door_knock_uint8], fmt='%i, ', header='{', footer='}', comments='')\n",
        "np.savetxt('clock_alarm_uint8.txt', [clock_alarm_uint8], fmt='%i, ', header='{', footer='}', comments='')\n",
        "np.savetxt('glass_breaking_uint8.txt', [glass_breaking_uint8], fmt='%i, ', header='{', footer='}', comments='')\n",
        "np.savetxt('fireworks_uint8.txt', [fireworks_uint8], fmt='%i, ', header='{', footer='}', comments='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRGhOz7mG1ov",
        "colab_type": "text"
      },
      "source": [
        "### run model with tflite interpreter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVnznPDUG9ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdK0rCZiHAuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "\n",
        "interpreter_quant.allocate_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNv8Dn1cHE2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dcd006a-41a4-4b4f-ed7b-f70b1fafb72b"
      },
      "source": [
        "for sound in urban_ds:\n",
        "  break\n",
        "\n",
        "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], sound)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor(\n",
        "    interpreter.get_output_details()[0][\"index\"])\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.26578802 0.0811587  0.45903614 0.04947668 0.01299273 0.13154782]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Vm65xxHID7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bba4d009-cc7f-406e-9161-0d2cd9736cf2"
      },
      "source": [
        "for sound in urban_ds_uint8:\n",
        "  break\n",
        "#print(sound)\n",
        "interpreter_quant.set_tensor(\n",
        "    interpreter_quant.get_input_details()[0][\"index\"], sound)\n",
        "interpreter_quant.invoke()\n",
        "predictions = interpreter_quant.get_tensor(\n",
        "    interpreter_quant.get_output_details()[0][\"index\"])\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   3   0 253]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}