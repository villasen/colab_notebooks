{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10-KWS-speech-Quantization-full.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villasen/colab_notebooks/blob/master/10_KWS_speech_Quantization_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-SiRp7S5Yds",
        "colab_type": "code",
        "outputId": "56faaced-08af-4eb7-a38e-b3d68169abf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "!git clone https://github.com/villasen/Speech-tiny.git\n",
        "!rm -r sample_data\n",
        "!rm Speech-tiny/.gitignore\n",
        "!rm -r Speech-tiny/.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Speech-tiny'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 54192 (delta 1), reused 6 (delta 1), pack-reused 54185\u001b[K\n",
            "Receiving objects: 100% (54192/54192), 1.40 GiB | 28.64 MiB/s, done.\n",
            "Resolving deltas: 100% (137/137), done.\n",
            "Checking out files: 100% (55253/55253), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixoHRO2P5dnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MFCC=10\n",
        "# If running models build locally\n",
        "LOCAL_MODELS = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NOsC0ii5iOW",
        "colab_type": "code",
        "outputId": "77b4025f-c3d1-4eb8-847b-5b58ed42f8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow\n",
        "import scipy\n",
        "import os, shutil\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from six.moves import urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import os.path\n",
        "from os import path\n",
        "from tensorflow.python.ops import io_ops\n",
        "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from numpy  import array\n",
        "from tensorflow.contrib.quantize.python import fold_batch_norms\n",
        "from tensorflow.contrib.quantize.python import quantize\n",
        "from tensorflow.python.framework import ops\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "\n",
        "########### FUNCTIONS\n",
        "\n",
        "def urban_wav2mfcc(file_path, max_pad_len):\n",
        "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=MFCC, n_fft=640, hop_length=320)\n",
        "      \n",
        "    pad_width = max_pad_len - mfcc.shape[1]\n",
        "    #print(max_pad_len)\n",
        "    #print(mfcc.shape[1])\n",
        "    #print(pad_width)\n",
        "    if pad_width < 0: \n",
        "      print(mfcc.shape[1])\n",
        "      print(pad_width)\n",
        "      print(\"error in \"+ file_path)\n",
        "    \n",
        "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    return mfcc \n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "def load_wav_file(wav_file):\n",
        "  with tf.Session(graph=tf.Graph()) as sess:\n",
        "      wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
        "      wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
        "      wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)\n",
        "      return sess.run(\n",
        "          wav_decoder,\n",
        "          feed_dict={wav_filename_placeholder: wav_file}).audio.flatten()\n",
        "\n",
        "\n",
        "def get_mfccs(file):\n",
        "  try:\n",
        "      wave, sr = librosa.load(wavfile , mono=True, sr=None)\n",
        "      mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=MFCC, n_fft=640, hop_length=320, norm='ortho')\n",
        "      mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
        "      \n",
        "  except Exception as error:\n",
        "    print(\"Error found handling file: \", file)\n",
        "    return None\n",
        "  \n",
        "  return mfcc_scaled\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AudioAugmentation:\n",
        "    def read_audio_file(self, file_path):\n",
        "        input_length = 16000\n",
        "        data = librosa.core.load(file_path)[0]\n",
        "        if len(data) > input_length:\n",
        "            data = data[:input_length]\n",
        "        else:\n",
        "            data = np.pad(data, (0, max(0, input_length - len(data))), \"constant\")\n",
        "        return data\n",
        "\n",
        "    def write_audio_file(self, file, data, sample_rate=16000):\n",
        "        librosa.output.write_wav(file, data, sample_rate)\n",
        "\n",
        "    def add_noise(self, data):\n",
        "            noise = np.random.randn(len(data))\n",
        "            data_noise = data + 0.005 * noise\n",
        "            return data_noise\n",
        "    def shift(self, data):\n",
        "            return np.roll(data, 1600)\n",
        "    def stretch(self, data, rate=1):\n",
        "            input_length = 16000\n",
        "            data = librosa.effects.time_stretch(data, rate)\n",
        "            if len(data) > input_length:\n",
        "                data = data[:input_length]\n",
        "            else:\n",
        "                data = np.pad(data, (0, max(0, input_length - len(data))), \"constant\")\n",
        "            return data\n",
        "    def plot_time_series(self, data):\n",
        "            fig = plt.figure(figsize=(10, 10))\n",
        "            plt.title('Raw wave ')\n",
        "            plt.ylabel('Amplitude')\n",
        "            plt.plot(np.linspace(0, 1, len(data)), data)\n",
        "            plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxCr6KmMEJEJ",
        "colab_type": "code",
        "outputId": "34f0ded2-fc62-43fd-fa8a-d37f1303401e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "\n",
        "\n",
        "DATA_PATH= '/content/Speech-tiny/dataset/'\n",
        "\n",
        "\n",
        "\n",
        "labels = os.listdir('Speech-tiny/dataset/')\n",
        "print(labels)\n",
        "\n",
        "test_single_file = '/content/test_single_file/'\n",
        "if path.exists('/content/test_single_file/') :\n",
        "    print (\"folder test_single exits, removing\")\n",
        "    !rm -rf /content/test_single_file\n",
        "os.mkdir(test_single_file)\n",
        "\n",
        "target_npy_files = \"/content/target_npy_files/\"\n",
        "if path.exists(\"/content/target_npy_files/\") :\n",
        "    print (\"folder target_npy_files exits, removing\")\n",
        "    !rm -rf /content/target_npy_files\n",
        "os.mkdir(target_npy_files)\n",
        "\n",
        "\n",
        "target_txt_files = \"/content/target_txt_files/\"\n",
        "if path.exists(\"/content/target_txt_files/\") :\n",
        "    print (\"folder target_txt_files folder exits, removing\")\n",
        "    !rm -rf /content/target_txt_files\n",
        "os.mkdir(target_txt_files)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# follow\n",
        "if path.exists(\"/content/test_single_file/follow\") :\n",
        "     print (\"folder test_single_file/follow exits, removing\")\n",
        "     !rm -rf /content/test_single_file/follow\n",
        "os.mkdir('/content/test_single_file/follow')\n",
        "\n",
        "\n",
        "\n",
        "# forward\n",
        "if path.exists(\"/content/test_single_file/forward\") :\n",
        "     print (\"folder test_single_file/forward exits, removing\")\n",
        "     !rm -rf /content/test_single_file/forward\n",
        "os.mkdir('/content/test_single_file/forward')\n",
        "\n",
        "# go\n",
        "if path.exists(\"/content/test_single_file/go\") :\n",
        "    print (\"folder test_single_file/go exits, removing\")\n",
        "    !rm -rf /content/test_single_file/go\n",
        "os.mkdir('/content/test_single_file/go')\n",
        "\n",
        "# yes\n",
        "if path.exists(\"/content/test_single_file/yes\") :\n",
        "    print (\"folder test_single_file/yes exits, removing\")\n",
        "    !rm -rf /content/test_single_file/yes\n",
        "os.mkdir('/content/test_single_file/yes')\n",
        "\n",
        "# no\n",
        "if path.exists(\"/content/test_single_file/no\") :\n",
        "    print (\"folder test_single_file/no exits, removing\")\n",
        "    !rm -rf /content/test_single_file/no\n",
        "os.mkdir('/content/test_single_file/no')\n",
        "\n",
        "\n",
        "# stop\n",
        "if path.exists(\"/content/test_single_file/stop\") :\n",
        "    print (\"folder test_single_file/stop exits, removing\")\n",
        "    !rm -rf /content/test_single_file/stop\n",
        "os.mkdir('/content/test_single_file/stop')\n",
        "\n",
        "\n",
        "# right\n",
        "if path.exists(\"/content/test_single_file/right\") :\n",
        "    print (\"folder test_single_file/right exits, removing\")\n",
        "    !rm -rf /content/test_single_file/right\n",
        "os.mkdir('/content/test_single_file/right')\n",
        "\n",
        "\n",
        "# left\n",
        "if path.exists(\"/content/test_single_file/left\") :\n",
        "    print (\"folder test_single_file/left exits, removing\")\n",
        "    !rm -rf /content/test_single_file/left\n",
        "os.mkdir('/content/test_single_file/left')\n",
        "\n",
        "\n",
        "# up\n",
        "if path.exists(\"/content/test_single_file/up\") :\n",
        "    print (\"folder test_single_file/up exits, removing\")\n",
        "    !rm -rf /content/test_single_file/up\n",
        "os.mkdir('/content/test_single_file/up')\n",
        "\n",
        "\n",
        "# down\n",
        "if path.exists(\"/content/test_single_file/down\") :\n",
        "    print (\"folder test_single_file/down exits, removing\")\n",
        "    !rm -rf /content/test_single_file/down\n",
        "os.mkdir('/content/test_single_file/down')\n",
        "\n",
        "\n",
        "if path.exists(\"/content/models/\") :\n",
        "    print (\"folder /content/models/ exits, removing\")\n",
        "    !rm -rf /content/models/\n",
        "os.mkdir('/content/models/')\n",
        "\n",
        "if path.exists(\"/content/models/speech\") :\n",
        "    print (\"folder /content/models/speech exits, removing\")\n",
        "    !rm -rf /content/models/speech/\n",
        "os.mkdir('/content/models/speech')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bed', 'off', 'yes', 'forward', 'dog', 'left', 'right', 'on', 'follow', 'up', 'sheila', 'learn', 'backward', 'house', 'cat', 'no', 'stop', 'go', 'down']\n",
            "folder test_single exits, removing\n",
            "folder target_npy_files exits, removing\n",
            "folder target_txt_files folder exits, removing\n",
            "folder /content/models/ exits, removing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgiVhX8xuJ3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create instance for augmentation\n",
        "aug = AudioAugmentation()\n",
        "\n",
        "# read file and show sound graph\n",
        "data = aug.read_audio_file(\"/content/Speech-tiny/dataset/forward/012187a4_nohash_0.wav\")\n",
        "aug.plot_time_series(data)\n",
        "\n",
        "# Add noise\n",
        "data_noise = aug.add_noise(data)\n",
        "aug.plot_time_series(data_noise)\n",
        "\n",
        "# shift sound\n",
        "data_shift = aug.shift(data)\n",
        "aug.plot_time_series(data_shift)\n",
        "\n",
        "# strech sound \n",
        "data_stretch = aug.stretch(data)\n",
        "aug.plot_time_series(data_stretch)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6oCq1PL2KX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Augmentation \n",
        "#Create instance for augmentation\n",
        "aug = AudioAugmentation()\n",
        "\n",
        "path = DATA_PATH\n",
        "labels = os.listdir(path) \n",
        "print(labels)\n",
        "i=0\n",
        "for label in labels:\n",
        "  mfcc_vectors=[]\n",
        "   \n",
        "  wavefiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "  if label=='follow' or label=='forward' or label=='go' or label=='yes' or label=='no' or label=='stop' or label=='right' or label=='left' or label=='up' or label=='down': \n",
        "    #print(wavefiles)\n",
        "    for wavfile in wavefiles:\n",
        "        wave, sr = librosa.load(wavfile , mono=True, sr=None)\n",
        "        data_noise = aug.add_noise(wave)\n",
        "        #data_files = np.pad(data_noise, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        #data_vectors.append(data_files)\n",
        "        num =str(i)\n",
        "        #np.save( wavfile + '_' + num, data_noise)\n",
        "        aug.write_audio_file( '/content/Speech-tiny/dataset/' + label + '/' + 'augmented_' + num + '.wav'  , data_noise)\n",
        "        i=i+1\n",
        "\n",
        "    print(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jnF_LsYF8Ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = DATA_PATH\n",
        "labels = os.listdir(path) \n",
        "print(labels)\n",
        "\n",
        "max_feature = 0\n",
        "for label in labels:\n",
        "  mfcc_vectors=[]\n",
        "   \n",
        "  wavefiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "    \n",
        "  if label=='follow' or label=='forward' or label=='go' or label=='yes' or label=='no' \\\n",
        "  or label=='stop' or label=='right' or label=='left' or label=='up' or label=='down':\n",
        "#  if label=='clapping': \n",
        "    \n",
        "    for wavfile in wavefiles:\n",
        "        max_pad_len = 51\n",
        "\n",
        "\n",
        "\n",
        "        wave, sr = librosa.load(wavfile , mono=True, sr=None)\n",
        "        mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=MFCC, n_fft=640, hop_length=320, norm='ortho')\n",
        "        #mfcc = np.mean(librosa.feature.mfcc(wave, sr=16000, n_mfcc=MFCC, n_fft=640, hop_length=320, norm='ortho'))\n",
        "\n",
        "\n",
        "\n",
        "        #print(mfcc)\n",
        "\n",
        "        pad_width = max_pad_len - mfcc.shape[1]\n",
        "\n",
        "        #print(max_pad_len)\n",
        "        #print(mfcc.shape[1])\n",
        "        #print(pad_width)\n",
        "\n",
        "        if pad_width < 0: \n",
        "          print(mfcc.shape[1])\n",
        "          print(pad_width)\n",
        "          print(\"error in \"+ file_path)\n",
        "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "        mfcc_vectors.append(mfcc)\n",
        "\n",
        "\n",
        "    np.save('/content/target_npy_files/' + label + '.npy', mfcc_vectors)\n",
        "    np.savetxt('/content/target_txt_files/' + label + '.txt', mfcc_vectors[2], delimiter=', ')\n",
        "    print(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li-UWW05xsrS",
        "colab_type": "code",
        "outputId": "88e300a5-c042-40c1-a6f9-b7e422609fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "!cp /content/Speech-tiny/npy_files_Augmented/*.npy /  /content/target_npy_files\n",
        "!cp /content/drive/My\\ Drive/ML_models/Tensorflow_lite/Speech/10_word_ESC50_split/speech-model.tflite   /content/models/speech/\n",
        "!cp /content/drive/My\\ Drive/ML_models/Tensorflow_lite/Speech/10_word_ESC50_split/speech_model_quant.tflite   /content/models/speech/\n",
        "!cp /content/drive/My\\ Drive/ML_models/Tensorflow_lite/Speech/10_word_ESC50_split/speech-model_quant_io.tflite   /content/models/speech/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: -r not specified; omitting directory '/'\n",
            "cp: cannot stat '/content/drive/My Drive/ML_models/Tensorflow_lite/Speech/10_word_ESC50_split/speech-model.tflite': No such file or directory\n",
            "cp: cannot stat '/content/drive/My Drive/ML_models/Tensorflow_lite/Speech/10_word_ESC50_split/speech_model_quant.tflite': No such file or directory\n",
            "cp: cannot stat '/content/drive/My Drive/ML_models/Tensorflow_lite/Speech/10_word_ESC50_split/speech-model_quant_io.tflite': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJPyH6T_1uvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_ratio = 0.9\n",
        "random_state = 42\n",
        "\n",
        "npy_files= os.listdir('/content/target_npy_files/')\n",
        "print(npy_files)\n",
        "\n",
        "#load from disk\n",
        "X = np.load('/content/target_npy_files/' + npy_files[0])\n",
        "y = np.zeros(X.shape[0])\n",
        "#print(npy_files[0])\n",
        "#print(npy_files[1])\n",
        "#print(npy_files[2])\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SqK7ugZ6FIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, label in enumerate(npy_files[1:]):\n",
        "   print(label)\n",
        "   print(i)\n",
        "   x = np.load('/content/target_npy_files/' + label)\n",
        "   #print(x)\n",
        "     \n",
        "\n",
        "   X = np.vstack((X, x))\n",
        "   print(X.shape)\n",
        "   \n",
        "   y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
        "   print(y.shape)\n",
        "   print(y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD0yf8RDTnHo",
        "colab_type": "code",
        "outputId": "07389e1a-562e-460e-ec96-f79b12ff8782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "split_ratio = 0.9\n",
        "random_state = 42\n",
        "\n",
        "npy_files= os.listdir('/content/target_npy_files/')\n",
        "print(npy_files)\n",
        "\n",
        "X = np.load('/content/target_npy_files/' + npy_files[0])\n",
        "y = np.zeros(X.shape[0])\n",
        "#print(npy_files[0])\n",
        "#print(npy_files[1])\n",
        "#print(npy_files[2])\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "\n",
        "\n",
        "# # Append all of the dataset into one single array\n",
        "for i, label in enumerate(npy_files[1:]):\n",
        "   x = np.load('/content/target_npy_files/' + label)\n",
        "  \n",
        "     \n",
        "\n",
        "   X = np.vstack((X, x))\n",
        "   y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
        "\n",
        "\n",
        "# return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True) \n",
        "\n",
        "\n",
        "\n",
        "X_std = (X - np.mean(X)) / (np.std(X))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
        "np.save('/content/X_test.npy', X_test)\n",
        "np.save('/content/y_test.npy', y_test)\n",
        "X_train = X_train.reshape(X_train.shape[0], MFCC, 51, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], MFCC, 51, 1)\n",
        "y_train_hot = to_categorical(y_train)\n",
        "y_test_hot = to_categorical(y_test)\n",
        "\n",
        "print(y_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['follow.npy', 'up.npy', 'left.npy', 'right.npy', 'forward.npy', 'no.npy', 'go.npy', 'yes.npy', 'stop.npy', 'down.npy']\n",
            "(3158, 10, 51)\n",
            "(3158,)\n",
            "[3. 1. 1. ... 2. 4. 7.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD54meC7ju_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeARD284Fgo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('/content/X_test', X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaOpg8D5aYJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_flat = X_train.flatten()\n",
        "X_train_max = max(X_train_flat)\n",
        "X_train_min = min(X_train_flat)\n",
        "\n",
        "print(X_train_max )\n",
        "print(X_train_min)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYRt8agPq7l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_urban_model():\n",
        " return keras.Sequential([\n",
        "# layer 1\n",
        "# Convolutional layer      \n",
        "    keras.layers.Conv2D(276, (4,MFCC), strides=(1,2), padding='same', activation='relu', input_shape=(MFCC,51,1), kernel_regularizer=keras.regularizers.l2(0.001)),     \n",
        "# layer 2    \n",
        "# Depthwise layer \n",
        "    keras.layers.SeparableConv2D(276, (3,3), strides=(2,2), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "# Separable layer \n",
        "    keras.layers.Conv2D(276, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "\n",
        "# layer 3\n",
        "# Depthwise layer \n",
        "    keras.layers.SeparableConv2D(276, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        " # Separable\n",
        "    keras.layers.Conv2D(276, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "\n",
        "# Average pool\n",
        "    keras.layers.AveragePooling2D(pool_size=(5, 12), strides=(2,2), padding='valid', data_format=None),\n",
        "# All in on vector\n",
        "    keras.layers.Flatten(data_format=None),\n",
        "# dropout to improve overfitting\n",
        "    keras.layers.Dropout(0.5),\n",
        " # Place all filters together   \n",
        "    keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001),  activation='relu'),\n",
        "# Predict my word        \n",
        "    keras.layers.Dense(10, activation='softmax')     \n",
        "      \n",
        "      \n",
        "  ])  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGjV8o8mu1Mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_urban_model():\n",
        " return keras.Sequential([\n",
        "      \n",
        "    keras.layers.Conv2D(64, (4,MFCC), strides=(2,2), padding='same', activation='relu', input_shape=(MFCC,51,1), kernel_regularizer=keras.regularizers.l2(0.001)),     \n",
        "\n",
        "# Depthwise layers\n",
        "    keras.layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "\n",
        "# Depthwise layers\n",
        "    keras.layers.SeparableConv2D(128, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Conv2D(128, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "\n",
        "# Depthwise layers\n",
        "    keras.layers.SeparableConv2D(128, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    keras.layers.Conv2D(128, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "\n",
        "    \n",
        "    keras.layers.AveragePooling2D(pool_size=(5, 25), strides=(2,2), padding='valid', data_format=None),\n",
        "    #keras.layers.Dropout(0.5),\n",
        "    keras.layers.Flatten(data_format=None),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(0.001),  activation='relu'),\n",
        "  \n",
        "      \n",
        "    keras.layers.Dense(10, activation='softmax')     \n",
        "      \n",
        "      \n",
        "  ])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew43HDH1qqgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -al target_npy_files\n",
        "!rm -rf target_npy_files/.ipynb_checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AvKGYs-Tp0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "\n",
        "#classes=[\"follow\", \"down\", \"left\", \"stop\", \"no\", \"yes\", \"up\", \"forward\", \"go\", \"right\"]\n",
        "#classes=['forward', 'no', 'stop', 'follow', 'up', 'go', 'left', 'down', 'right', 'yes']\n",
        "classes=['down', 'right', 'stop', 'yes', 'forward', 'follow', 'left', 'up', 'no', 'go']\n",
        "!rm checkpoint\n",
        "!rm checkpoints.data-00000-of-00001\n",
        "!rm checkpoints.index\n",
        "!rm checkpoints.meta\n",
        "\n",
        "\n",
        "speech_graph = tf.Graph()\n",
        "speech_sess = tf.Session(graph=speech_graph)\n",
        "\n",
        "keras.backend.set_session(speech_sess)\n",
        "with speech_graph.as_default():\n",
        "     \n",
        "  #build my model\n",
        "  model = build_urban_model()\n",
        "  #give me model structure\n",
        "  model.summary()\n",
        "\n",
        "  #my own optimizer\n",
        "  Amartin = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999)\n",
        "  \n",
        "  model.compile(optimizer= \"Adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  \n",
        "    \n",
        "  # run it\n",
        "  history = model.fit(X_train, y_train_hot, batch_size=200, epochs=100, verbose=1, validation_data=(X_test, y_test_hot))\n",
        "\n",
        "\n",
        "\n",
        "  # y_pred=model.predict_classes(X_test)\n",
        "  # con_mat = tf.math.confusion_matrix(labels=y_true, predictions=y_pred).numpy()   \n",
        "  # real_label_batch = tf.argmax(y_test_hot, axis=1)\n",
        "  # y_pred=model.predict_classes(X_test)\n",
        "  # con_mat = tf.math.confusion_matrix(labels=real_label_batch, predictions=y_pred).numpy()   \n",
        "\n",
        "  # con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "  # con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "  #                      index = classes, \n",
        "  #                      columns = classes)\n",
        "\n",
        "  # figure = plt.figure(figsize=(6, 6))\n",
        "  # sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "  # plt.tight_layout()\n",
        "  # plt.ylabel('True label')\n",
        "  # plt.xlabel('Predicted label')\n",
        "  # plt.show()\n",
        "  \n",
        "  #save my graph  \n",
        "  saver = tf.train.Saver()\n",
        "  saver.save(speech_sess, 'checkpoints')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-GhhICUTvxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes=[\"follow\", \"down\", \"left\", \"stop\", \"no\", \"yes\", \"up\", \"forward\", \"go\", \"right\"]\n",
        "real_label_batch = tf.argmax(y_test_hot, axis=1)\n",
        "y_pred=model.predict_classes(X_test)\n",
        "con_mat = tf.math.confusion_matrix(labels=real_label_batch, predictions=y_pred).numpy()   \n",
        "\n",
        "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "con_mat_df = pd.DataFrame(con_mat_norm,\n",
        "                     index = classes, \n",
        "                     columns = classes)\n",
        "\n",
        "figure = plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "tf.compat.v1.logging.info('Confusion Matrix:\\n %s' % (con_mat))\n",
        "tf.compat.v1.logging.info('Confusion Matrix Normalized:\\n %s' % (con_mat_norm))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHP9t6ZiTy8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs,val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpdOuXb6bYqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_speech_graph = tf.Graph()\n",
        "eval_speech_sess = tf.Session(graph=eval_speech_graph)\n",
        "\n",
        "keras.backend.set_session(eval_speech_sess)\n",
        "with eval_speech_graph.as_default():\n",
        "    keras.backend.set_learning_phase(0)\n",
        "    eval_model = build_urban_model()\n",
        "\n",
        "    #For quantization aware training only\n",
        "    #tf.contrib.quantize.create_eval_graph(input_graph=eval_speech_graph)\n",
        "    eval_speech_graph_def = eval_speech_graph.as_graph_def()\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(eval_speech_sess, 'checkpoints')\n",
        "\n",
        "    frozen_graph_def = tf.graph_util.convert_variables_to_constants( eval_speech_sess, eval_speech_graph_def, \n",
        "                                                                    [eval_model.output.op.name] )\n",
        "\n",
        "    with open('speech_model.pb', 'wb') as f:\n",
        "      f.write(frozen_graph_def.SerializeToString())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEIHc75apes8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_frozen_graph( \"speech_model.pb\", [\"conv2d_1_input\"], [\"dense_2/Softmax\"])\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ1lG4cGqUDZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tflite_models_dir = pathlib.Path(\"/tmp/ESC50-split-models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1qd9qYAqfha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tflite_model_file = tflite_models_dir/\"speech-model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws6gRK-vqkOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Tg2QTGql5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimize for size\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Azo2UcqrFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create quantized values with an accurate dynamic range of activations, \n",
        "# for that need to provide a representative dataset\n",
        "\n",
        "#sounds = tf.cast(X_train, tf.float32)/1000.0\n",
        "sounds = tf.cast(X_train, tf.float32)\n",
        "urban_ds = tf.data.Dataset.from_tensor_slices((sounds)).batch(1)\n",
        "def representative_data_gen():\n",
        "  for input_value in urban_ds.take(100):\n",
        "    yield [input_value]\n",
        "    \n",
        "converter.representative_dataset = representative_data_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjIxPyEyqxEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tflite format quantized\n",
        "tflite_quant_model = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"speech_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_quant_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3cM67i9rMOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFsTYxzErPEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The converted model needs to be fully quantized. That means all ops need to be \n",
        "# quantized, no floats left. The input and outputs need to be integers too.\n",
        "\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"speech-model_quant_io.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ks5W7qNrYm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qLszLvdraG0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "ded0dc83-5c0e-4875-dc30-d626f7623ea3"
      },
      "source": [
        "# Load data for float model\n",
        "sounds = tf.cast(X_test, tf.float32)\n",
        "print(sounds.shape)\n",
        "urban_ds = tf.data.Dataset.from_tensor_slices(sounds).batch(1)\n",
        "#print(sounds)\n",
        "\n",
        "X_train_flat = X_train.flatten()\n",
        "X_train_max = max(X_train_flat)\n",
        "X_train_min = min(X_train_flat)\n",
        "print(X_train_max)\n",
        "print(X_train_min)\n",
        "\n",
        "\n",
        "# Load data for quantized model\n",
        "X_test_flat = X_test.flatten()\n",
        "X_test_max = max(X_test_flat)\n",
        "X_test_min = min(X_test_flat)\n",
        "print(X_test_max)\n",
        "print(X_test_min)\n",
        "\n",
        "quant_sounds = tf.quantization.quantize(X_test, X_train_min, X_train_max, tf.quint8, mode=\"MIN_COMBINED\", round_mode=\"HALF_AWAY_FROM_ZERO\", name=None)\n",
        "#quant_sounds = tf.quantization.quantize(X_test, X_test_min, X_test_max, tf.quint8, mode=\"MIN_COMBINED\", round_mode=\"HALF_AWAY_FROM_ZERO\", name=None)\n",
        "sounds_uint8 = tf.cast(quant_sounds[0], tf.uint8)\n",
        "urban_ds_uint8 = tf.data.Dataset.from_tensor_slices(sounds_uint8).batch(1)\n",
        "\n",
        "#print(sounds_uint8)\n",
        "#print(X_test)\n",
        "#print(quant_sounds[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6819, 10, 51, 1)\n",
            "2.58073663032399\n",
            "-6.37757019552536\n",
            "2.5334635916673305\n",
            "-6.4010885108383695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jgdU0tDtwh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "863970e2-7938-4c6a-c5ee-85376d94d4f0"
      },
      "source": [
        "if LOCAL_MODELS == False :\n",
        "  if path.exists(\"/content/Speech-tiny/Models/speech/\"):\n",
        "    tflite_model=\"/content/Speech-tiny/Models/speech/speech-model.tflite\"\n",
        "    print('running external non-quantized speech model')\n",
        "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model))\n",
        "    interpreter.allocate_tensors()\n",
        "else:\n",
        "  print('running local non-quantized model')\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "  interpreter.allocate_tensors()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running external non-quantized speech model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySK1GhYJtzwe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4ce10b2-853b-4fd4-ac57-06a77da005c8"
      },
      "source": [
        "if LOCAL_MODELS == False :\n",
        "  if path.exists(\"/content/Speech-tiny/Models/speech/\"):\n",
        "    tflite_model=\"/content/Speech-tiny/Models/speech/speech-model_quant_io.tflite\"\n",
        "    print('running external quantized speech model')\n",
        "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model))\n",
        "    interpreter.allocate_tensors()\n",
        "else:\n",
        "  print('running local quantized model')\n",
        "  interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "  interpreter_quant.allocate_tensors()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running external quantized speech model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEPRWIRtvVYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(records, n):\n",
        "  max = records[0]\n",
        "  \n",
        "  for i in range(1, n):\n",
        "    if records[i] > max:\n",
        "      max = records[i]\n",
        "\n",
        "  index = record.index(max)\n",
        "\n",
        "  return max, index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG4uyvirt3f1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#labels = ['forward', 'no', 'stop', 'follow', 'up', 'go', 'left', 'down', 'right', 'yes']\n",
        "\n",
        "labels=['left', 'right', 'go', 'no', 'follow', 'up', 'stop', 'yes', 'down', 'forward']\n",
        "\n",
        "\n",
        "for m in range(11, 30):\n",
        "  print(\"\\nRunning number\", m)\n",
        "  predict_number = m\n",
        "  i=0\n",
        "  for sound in urban_ds:\n",
        "    #for list in sound:\n",
        "      \n",
        "      #break\n",
        "    if i == predict_number:\n",
        "      break\n",
        "    i= i+1\n",
        "    \n",
        "    #print(list[1])\n",
        "  #print(sound.shape)\n",
        "  #print(list.shape)\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], sound)\n",
        "  interpreter.invoke()\n",
        "  predictions = interpreter.get_tensor(\n",
        "      interpreter.get_output_details()[0][\"index\"])\n",
        "\n",
        "  print(predictions)\n",
        "\n",
        "  class_prediction = predictions.tolist()\n",
        "\n",
        "  for record in class_prediction:\n",
        "    record\n",
        "\n",
        "  records = array(record)\n",
        "\n",
        "  n = len(records)\n",
        "\n",
        "  class_predicted, indice = predict(records, n)\n",
        "\n",
        "  real_words = y_test[predict_number]\n",
        "  real_word = int(real_words)\n",
        "  word_predicted = labels[indice]\n",
        "\n",
        "  print(labels)\n",
        "  print(\"\\nFLOATS\")\n",
        "  print('---------------------')\n",
        "  print('Predicted class is ', labels[indice])\n",
        "  print('Real class is ', labels[real_word])\n",
        "  print('----------------------\\n\\n')\n",
        "\n",
        "\n",
        "  i=0\n",
        "  for sound in urban_ds_uint8:\n",
        "    \n",
        "    if i == predict_number:\n",
        "      break\n",
        "    i= i+1\n",
        "  #print(sound)\n",
        "  interpreter_quant.set_tensor(interpreter_quant.get_input_details()[0][\"index\"], sound)\n",
        "  interpreter_quant.invoke()\n",
        "  predictions = interpreter_quant.get_tensor(\n",
        "      interpreter_quant.get_output_details()[0][\"index\"])\n",
        "\n",
        "\n",
        "  class_prediction = predictions.tolist()\n",
        "\n",
        "  for record in class_prediction:\n",
        "    record\n",
        "\n",
        "  records = array(record)\n",
        "\n",
        "  n = len(records)\n",
        "\n",
        "  class_predicted, indice = predict(records, n)\n",
        "\n",
        "  print(predictions)\n",
        "\n",
        "  print(\"\\n\\nINTEGERS\")\n",
        "  print(\"---------------------------\")\n",
        "\n",
        "  print('Predicted class is ', labels[indice])\n",
        "  print('Real class is ', labels[real_word])\n",
        "  print('----------------------------')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}