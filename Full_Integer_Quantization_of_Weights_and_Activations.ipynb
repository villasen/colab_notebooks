{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full-Integer-Quantization-of-Weights-and-Activations.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villasen/colab_notebooks/blob/master/Full_Integer_Quantization_of_Weights_and_Activations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FZBJ-Ro4Dsn",
        "colab_type": "text"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Ozn5Bu3tfV",
        "colab_type": "code",
        "outputId": "c959960b-9737-404d-dc9c-183eb22dac48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!git clone https://github.com/villasen/small-urban-sound-dataset.git\n",
        "!rm -r sample_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'small-urban-sound-dataset'...\n",
            "remote: Enumerating objects: 9724, done.\u001b[K\n",
            "remote: Counting objects: 100% (9724/9724), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9328/9328), done.\u001b[K\n",
            "remote: Total 9724 (delta 400), reused 9716 (delta 394), pack-reused 0\n",
            "Receiving objects: 100% (9724/9724), 259.08 MiB | 13.89 MiB/s, done.\n",
            "Resolving deltas: 100% (400/400), done.\n",
            "Checking out files: 100% (18530/18530), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MV0wn6g4Kiu",
        "colab_type": "text"
      },
      "source": [
        "### Load Libraries, utilites and Function definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZNPWdAm4XdR",
        "colab_type": "code",
        "outputId": "507b6de0-7acb-4c00-c2b4-f797d1ee07e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow\n",
        "import scipy\n",
        "import os, shutil\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from six.moves import urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import os.path\n",
        "from os import path\n",
        "from tensorflow.python.ops import io_ops\n",
        "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
        "\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from numpy  import array\n",
        "from tensorflow.contrib.quantize.python import fold_batch_norms\n",
        "from tensorflow.contrib.quantize.python import quantize\n",
        "from tensorflow.python.framework import ops\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########### FUNCTIONS\n",
        "\n",
        "def urban_wav2mfcc(file_path, max_pad_len):\n",
        "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
        "    #mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=3200, hop_length=1600)\n",
        "    mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=640, hop_length=320)\n",
        "    \n",
        "    pad_width = max_pad_len - mfcc.shape[1]\n",
        "    #print(max_pad_len)\n",
        "    #print(mfcc.shape[1])\n",
        "    #print(pad_width)\n",
        "    if pad_width < 0: \n",
        "      print(mfcc.shape[1])\n",
        "      print(pad_width)\n",
        "      print(\"error in \"+ file_path)\n",
        "    \n",
        "    mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    return mfcc \n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "def load_wav_file(wav_file):\n",
        "  with tf.Session(graph=tf.Graph()) as sess:\n",
        "      wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
        "      wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
        "      wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)\n",
        "      return sess.run(\n",
        "          wav_decoder,\n",
        "          feed_dict={wav_filename_placeholder: wav_file}).audio.flatten()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def quantize_wav(wav_tensor):\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        my_quant = tf.quantization.quantize(\n",
        "            # input\n",
        "            wav_tensor,\n",
        "            #min_range    -32768\n",
        "            -1.0,\n",
        "            # max_range    32677\n",
        "            1.0,\n",
        "            # T\n",
        "            tf.qint16,\n",
        "            # modes . 'MIN_COMBINED', \"MIN_FIRST\", \"SCALED\"\n",
        "            #mode='SCALED',\n",
        "            mode=\"MIN_COMBINED\",\n",
        "            #mode=\"MIN_FIRST\",\n",
        "            # round_mode='HALF_AWAY_FROM_ZERO', 'HALF_TO_EVEN'\n",
        "            round_mode='HALF_AWAY_FROM_ZERO',\n",
        "            #round_mode='HALF_TO_EVEN',\n",
        "            name=None)\n",
        "        return sess.run(my_quant).output\n",
        "  \n",
        "  \n",
        "  \n",
        "def build_speech_model():\n",
        "  return keras.Sequential([\n",
        "      \n",
        "    keras.layers.Conv2D(64, (4,10), strides=(2,2), padding='same', activation='relu', input_shape=(10,51,1)),     \n",
        "    #keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0),\n",
        "# Depthwise layers\n",
        "    keras.layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'),\n",
        "    keras.layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "\n",
        "#    keras.layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'),\n",
        "#    keras.layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu'),\n",
        "#    keras.layers.Dropout(0.5),\n",
        "\n",
        "#    keras.layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'),\n",
        "    \n",
        "#    keras.layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu'),\n",
        "#    keras.layers.Dropout(0.5),  \n",
        "#    keras.layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'),\n",
        "    \n",
        "#     keras.layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False, activation='relu'),\n",
        "      \n",
        "      \n",
        "    keras.layers.AveragePooling2D(pool_size=(5, 25), strides=(2,2), padding='valid', data_format=None),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Flatten(data_format=None),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "      \n",
        "    keras.layers.Dense(3, activation='softmax')     \n",
        "      \n",
        "      \n",
        "  ])  \n",
        "\n",
        "\n",
        "######################\n",
        "\n",
        "# Installing tf version 1.15\n",
        "! pip uninstall -y tensorflow\n",
        "! pip install -U tf-nightly\n",
        "\n",
        "!rm -rf /tmp/urban_sound_models\n",
        "# !pip install -q tensorflow==2.0.0-rc1\n",
        "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "# import tensorflow as tf\n",
        "# print(tf.__version__)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0-dev20190821\n",
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\n",
            "Requirement already up-to-date: tf-nightly in /usr/local/lib/python3.6/dist-packages (1.15.0.dev20190821)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.7.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: tb-nightly<1.16.0a0,>=1.15.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0a20190911)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.1.7)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tf-estimator-nightly in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.0.0.dev2019093001)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly) (41.2.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhzelde35Wul",
        "colab_type": "code",
        "outputId": "621e750d-8cd2-4c09-d074-84a575c9740a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        }
      },
      "source": [
        "#############  Preparing input data\n",
        "\n",
        "\n",
        "DATA_PATH= '/content/small-urban-sound-dataset/tiny-dataset/'\n",
        "!ls -al small-urban-sound-dataset/tiny-dataset/\n",
        "\n",
        "\n",
        "labels = os.listdir('small-urban-sound-dataset/tiny-dataset/')\n",
        "print(labels)\n",
        "\n",
        "\n",
        "test_single_file = '/content/test_single_file/'\n",
        "if path.exists('/content/test_single_file/') :\n",
        "    print (\"folder test_single exits, removing\")\n",
        "    !rm -rf /content/test_single_file\n",
        "os.mkdir(test_single_file)\n",
        "\n",
        "\n",
        "target_npy_files = \"/content/target_npy_files/\"\n",
        "if path.exists(\"/content/target_npy_files/\") :\n",
        "    print (\"folder target_npy_files exits, removing\")\n",
        "    !rm -rf /content/target_npy_files\n",
        "os.mkdir(target_npy_files)\n",
        "\n",
        "\n",
        "target_txt_files = \"/content/target_txt_files/\"\n",
        "if path.exists(\"/content/target_txt_files/\") :\n",
        "    print (\"folder target_txt_files folder exits, removing\")\n",
        "    !rm -rf /content/target_txt_files\n",
        "os.mkdir(target_txt_files)\n",
        "\n",
        "\n",
        "if path.exists(\"/content/test_single_file/frog\") :\n",
        "    print (\"folder test_single_file/frog exits, removing\")\n",
        "    !rm -rf /content/test_single_file/frog\n",
        "os.mkdir('/content/test_single_file/frog')\n",
        "\n",
        "if path.exists(\"/content/test_single_file/crow\") :\n",
        "    print (\"folder test_single_file/crow exits, removing\")\n",
        "    !rm -rf /content/test_single_file/crow\n",
        "os.mkdir('/content/test_single_file/crow')\n",
        "\n",
        "if path.exists(\"/content/test_single_file/crickets\") :\n",
        "    print (\"folder test_single_file/crickets exits, removing\")\n",
        "    !rm -rf /content/test_single_file/crickets\n",
        "os.mkdir('/content/test_single_file/crickets')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!cp -r /content/small-urban-sound-dataset/tiny-dataset/frog/1-18755-B.wav  /content/test_single_file/frog/\n",
        "!cp -r /content/small-urban-sound-dataset/tiny-dataset/crow/2-108767-C.wav  /content/test_single_file/crow/\n",
        "!cp -r /content/small-urban-sound-dataset/tiny-dataset/crickets/1-57318-A.wav  /content/test_single_file/crickets/\n",
        "\n",
        "\n",
        "\n",
        "path = DATA_PATH\n",
        "labels = os.listdir(path) \n",
        "print(labels)\n",
        "print(type(labels))\n",
        "max_feature = 0\n",
        "for label in labels:\n",
        "  mfcc_vectors=[]\n",
        "   \n",
        "  wavefiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
        "    \n",
        "  if label=='crow' or label=='frog' or label=='crickets': \n",
        "    for wavfile in wavefiles:\n",
        "            \n",
        "      max_pad_len = 51\n",
        "      #mfcc = urban_wav2mfcc(wavfile, 51)\n",
        "      wave, sr = librosa.load(wavfile , mono=True, sr=None)\n",
        "      #mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=3200, hop_length=1600)\n",
        "      mfcc = librosa.feature.mfcc(wave, sr=16000, n_mfcc=10, n_fft=640, hop_length=320)\n",
        "\n",
        "      pad_width = max_pad_len - mfcc.shape[1]\n",
        "      #print(max_pad_len)\n",
        "      #print(mfcc.shape[1])\n",
        "      #print(pad_width)\n",
        "      if pad_width < 0: \n",
        "        print(mfcc.shape[1])\n",
        "        print(pad_width)\n",
        "        print(\"error in \"+ file_path)\n",
        "      mfcc_vectors.append(mfcc)\n",
        "\n",
        "\n",
        "    np.save('/content/target_npy_files/' + label + '.npy', mfcc_vectors)\n",
        "    np.savetxt('/content/target_txt_files/' + label + '.txt', mfcc_vectors[2], delimiter=', ')\n",
        "    print(label)\n",
        "    \n",
        "    \n",
        "split_ratio = 0.8\n",
        "random_state = 42\n",
        "\n",
        "npy_files= os.listdir('/content/target_npy_files/')\n",
        "print(npy_files)\n",
        "\n",
        "X = np.load('/content/target_npy_files/' + npy_files[0])\n",
        "y = np.zeros(X.shape[0])\n",
        "print(npy_files[0])\n",
        "print(X.shape)\n",
        "\n",
        "\n",
        "\n",
        "# # Append all of the dataset into one single array, same goes for \n",
        "for i, label in enumerate(npy_files[1:]):\n",
        "   x = np.load('/content/target_npy_files/' + label)\n",
        "  \n",
        "#     #x = np.load('/content/speech-numpy/' + label)\n",
        "\n",
        "   X = np.vstack((X, x))\n",
        "   y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))\n",
        "\n",
        "\n",
        "# return train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True) \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
        "\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 10, 51, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 10, 51, 1)\n",
        "y_train_hot = to_categorical(y_train)\n",
        "y_test_hot = to_categorical(y_test)\n",
        "\n",
        "\n",
        "mfcc_crow = urban_wav2mfcc('test_single_file/crow/2-108767-C.wav', max_pad_len=51)\n",
        "mfcc_crickets = urban_wav2mfcc('test_single_file/crickets/1-57318-A.wav', max_pad_len=51)\n",
        "mfcc_frog = urban_wav2mfcc('test_single_file/frog/1-18755-B.wav', max_pad_len=51)\n",
        "\n",
        "\n",
        "crow_file = mfcc_crow.reshape(1, 10, 51, 1)\n",
        "crickets_file = mfcc_crickets.reshape(1, 10, 51, 1)\n",
        "frog_file = mfcc_frog.reshape(1, 10, 51, 1)\n",
        "\n",
        "\n",
        "np.save('/content/' + 'crow_single.npy', crow_file)\n",
        "np.save('/content/' + 'frog_single.npy', frog_file)\n",
        "np.save('/content/' + 'crickets_single.npy', crickets_file)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 56\n",
            "drwxr-xr-x 14 root root 4096 Oct  1 03:08 .\n",
            "drwxr-xr-x  6 root root 4096 Oct  1 03:08 ..\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 crackling_fire\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 crickets\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 crow\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 crying_baby\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 door_knock\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 door_wood_creacks\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 drinking_sipping\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 fireworks\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 footsteps\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 frog\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 glass_breaking\n",
            "drwxr-xr-x  2 root root 4096 Oct  1 03:08 handsaw\n",
            "['handsaw', 'door_wood_creacks', 'glass_breaking', 'door_knock', 'crying_baby', 'fireworks', 'drinking_sipping', 'crackling_fire', 'frog', 'footsteps', 'crow', 'crickets']\n",
            "folder test_single exits, removing\n",
            "folder target_npy_files exits, removing\n",
            "folder target_txt_files folder exits, removing\n",
            "['handsaw', 'door_wood_creacks', 'glass_breaking', 'door_knock', 'crying_baby', 'fireworks', 'drinking_sipping', 'crackling_fire', 'frog', 'footsteps', 'crow', 'crickets']\n",
            "<class 'list'>\n",
            "frog\n",
            "crow\n",
            "crickets\n",
            "['frog.npy', 'crow.npy', 'crickets.npy']\n",
            "frog.npy\n",
            "(40, 10, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYlu00J45tqZ",
        "colab_type": "code",
        "outputId": "8efd1798-c7d4-4892-cd76-80035561dc67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "######### Here is where we start to build the model\n",
        "\n",
        "!rm checkpoint\n",
        "!rm checkpoints.data-00000-of-00001\n",
        "!rm checkpoints.index\n",
        "!rm checkpoints.meta\n",
        "\n",
        "\n",
        "speech_graph = tf.Graph()\n",
        "speech_sess = tf.Session(graph=speech_graph)\n",
        "\n",
        "keras.backend.set_session(speech_sess)\n",
        "with speech_graph.as_default():\n",
        "     \n",
        "  model = build_speech_model()\n",
        "  \n",
        "  # Quantization Aware Training #####\n",
        "  ##tf.contrib.quantize.create_training_graph(sess.graph)  <--- this is old\n",
        "  \n",
        "  #tf.contrib.quantize.create_training_graph(input_graph=speech_graph, quant_delay=20)\n",
        "  \n",
        "  ##sess.run(tf.global_variables_initializer()) \n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#  tf.contrib.quantize.create_training_graph(input_graph=speech_graph, quant_delay=2000000)\n",
        "#  speech_sess.run(tf.global_variables_initializer())\n",
        "  ####################\n",
        "  \n",
        "# LOSS FUNCTIONS:\n",
        "# categorical_crossentropy\n",
        "# logcosh\n",
        "# cosine_proximity\n",
        "# poisson\n",
        "\n",
        "# OPTIMIZERS:\n",
        "# adam\n",
        "# adamax\n",
        "# SGD\n",
        "# RMSprop\n",
        "# Adagrad\n",
        "# Adadelta\n",
        "  \n",
        "  model.compile(optimizer='adamax', loss='logcosh', metrics=['accuracy'])\n",
        "  \n",
        "  history = model.fit(X_train, y_train_hot, batch_size=10, epochs=200, verbose=1, validation_data=(X_test, y_test_hot))\n",
        "  #history = model.fit(X_train, y_train_hot, epochs=3)\n",
        "  saver = tf.train.Saver()\n",
        "  saver.save(speech_sess, 'checkpoints')\n",
        "  \n",
        "  acc = history.history['acc']\n",
        "  val_acc = history.history['val_acc']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "\n",
        "  plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "  plt.plot(epochs,val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  final_prediction1 = model.predict(frog_file)\n",
        "  final_prediction2 =  model.predict(crow_file)\n",
        "  final_prediction3 =  model.predict(crickets_file)\n",
        "  \n",
        "  \n",
        "  \n",
        "  print(npy_files)\n",
        "  print(final_prediction1)\n",
        "  print(final_prediction2)\n",
        "  print(final_prediction3)\n",
        "#########  End of training"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 96 samples, validate on 24 samples\n",
            "Epoch 1/200\n",
            "96/96 [==============================] - 1s 6ms/step - loss: 0.1239 - acc: 0.3125 - val_loss: 0.1016 - val_acc: 0.4167\n",
            "Epoch 2/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0952 - acc: 0.4688 - val_loss: 0.0959 - val_acc: 0.4167\n",
            "Epoch 3/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0972 - acc: 0.4063 - val_loss: 0.0929 - val_acc: 0.5000\n",
            "Epoch 4/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0804 - acc: 0.5521 - val_loss: 0.0910 - val_acc: 0.5000\n",
            "Epoch 5/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0827 - acc: 0.5625 - val_loss: 0.0878 - val_acc: 0.5000\n",
            "Epoch 6/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0834 - acc: 0.5313 - val_loss: 0.0830 - val_acc: 0.6250\n",
            "Epoch 7/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0751 - acc: 0.6354 - val_loss: 0.0832 - val_acc: 0.5417\n",
            "Epoch 8/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0683 - acc: 0.6562 - val_loss: 0.0818 - val_acc: 0.6250\n",
            "Epoch 9/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0702 - acc: 0.6458 - val_loss: 0.0807 - val_acc: 0.6250\n",
            "Epoch 10/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0769 - acc: 0.5729 - val_loss: 0.0834 - val_acc: 0.5417\n",
            "Epoch 11/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0721 - acc: 0.6042 - val_loss: 0.0869 - val_acc: 0.4583\n",
            "Epoch 12/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0706 - acc: 0.6042 - val_loss: 0.0805 - val_acc: 0.6250\n",
            "Epoch 13/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0613 - acc: 0.6875 - val_loss: 0.0781 - val_acc: 0.6250\n",
            "Epoch 14/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0682 - acc: 0.6354 - val_loss: 0.0789 - val_acc: 0.6250\n",
            "Epoch 15/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0605 - acc: 0.7188 - val_loss: 0.0865 - val_acc: 0.5417\n",
            "Epoch 16/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0603 - acc: 0.6979 - val_loss: 0.0805 - val_acc: 0.6250\n",
            "Epoch 17/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0532 - acc: 0.7396 - val_loss: 0.0758 - val_acc: 0.6250\n",
            "Epoch 18/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0612 - acc: 0.6875 - val_loss: 0.0787 - val_acc: 0.6250\n",
            "Epoch 19/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0647 - acc: 0.6458 - val_loss: 0.0798 - val_acc: 0.6250\n",
            "Epoch 20/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0574 - acc: 0.6979 - val_loss: 0.0793 - val_acc: 0.5833\n",
            "Epoch 21/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0586 - acc: 0.7292 - val_loss: 0.0744 - val_acc: 0.5833\n",
            "Epoch 22/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0462 - acc: 0.8125 - val_loss: 0.0725 - val_acc: 0.6250\n",
            "Epoch 23/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0480 - acc: 0.7813 - val_loss: 0.0686 - val_acc: 0.6667\n",
            "Epoch 24/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0502 - acc: 0.7812 - val_loss: 0.0681 - val_acc: 0.6667\n",
            "Epoch 25/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0449 - acc: 0.8021 - val_loss: 0.0729 - val_acc: 0.6250\n",
            "Epoch 26/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0439 - acc: 0.8125 - val_loss: 0.0740 - val_acc: 0.6250\n",
            "Epoch 27/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0412 - acc: 0.8229 - val_loss: 0.0643 - val_acc: 0.6667\n",
            "Epoch 28/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0450 - acc: 0.7917 - val_loss: 0.0646 - val_acc: 0.6667\n",
            "Epoch 29/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0466 - acc: 0.7917 - val_loss: 0.0652 - val_acc: 0.6667\n",
            "Epoch 30/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0380 - acc: 0.8542 - val_loss: 0.0673 - val_acc: 0.6250\n",
            "Epoch 31/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0345 - acc: 0.8646 - val_loss: 0.0701 - val_acc: 0.6250\n",
            "Epoch 32/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0406 - acc: 0.8437 - val_loss: 0.0695 - val_acc: 0.6250\n",
            "Epoch 33/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0361 - acc: 0.8229 - val_loss: 0.0570 - val_acc: 0.7500\n",
            "Epoch 34/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0379 - acc: 0.8125 - val_loss: 0.0597 - val_acc: 0.6667\n",
            "Epoch 35/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0411 - acc: 0.8333 - val_loss: 0.0602 - val_acc: 0.7083\n",
            "Epoch 36/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0355 - acc: 0.8646 - val_loss: 0.0613 - val_acc: 0.6667\n",
            "Epoch 37/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0344 - acc: 0.8542 - val_loss: 0.0653 - val_acc: 0.6667\n",
            "Epoch 38/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0326 - acc: 0.8646 - val_loss: 0.0597 - val_acc: 0.7083\n",
            "Epoch 39/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0346 - acc: 0.8750 - val_loss: 0.0554 - val_acc: 0.7500\n",
            "Epoch 40/200\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0280 - acc: 0.8958 - val_loss: 0.0578 - val_acc: 0.7500\n",
            "Epoch 41/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0351 - acc: 0.8437 - val_loss: 0.0521 - val_acc: 0.7917\n",
            "Epoch 42/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0308 - acc: 0.8750 - val_loss: 0.0557 - val_acc: 0.7500\n",
            "Epoch 43/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0279 - acc: 0.8542 - val_loss: 0.0509 - val_acc: 0.8333\n",
            "Epoch 44/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0352 - acc: 0.8646 - val_loss: 0.0526 - val_acc: 0.8333\n",
            "Epoch 45/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0270 - acc: 0.8750 - val_loss: 0.0571 - val_acc: 0.7500\n",
            "Epoch 46/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0288 - acc: 0.8646 - val_loss: 0.0490 - val_acc: 0.8333\n",
            "Epoch 47/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0371 - acc: 0.8542 - val_loss: 0.0549 - val_acc: 0.7917\n",
            "Epoch 48/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0253 - acc: 0.9167 - val_loss: 0.0528 - val_acc: 0.7917\n",
            "Epoch 49/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0235 - acc: 0.9062 - val_loss: 0.0639 - val_acc: 0.7083\n",
            "Epoch 50/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0236 - acc: 0.9062 - val_loss: 0.0454 - val_acc: 0.7917\n",
            "Epoch 51/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0288 - acc: 0.8646 - val_loss: 0.0487 - val_acc: 0.7917\n",
            "Epoch 52/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0242 - acc: 0.8958 - val_loss: 0.0516 - val_acc: 0.8333\n",
            "Epoch 53/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0278 - acc: 0.8646 - val_loss: 0.0604 - val_acc: 0.7500\n",
            "Epoch 54/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0238 - acc: 0.9167 - val_loss: 0.0520 - val_acc: 0.8333\n",
            "Epoch 55/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0282 - acc: 0.8750 - val_loss: 0.0546 - val_acc: 0.7917\n",
            "Epoch 56/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0263 - acc: 0.8646 - val_loss: 0.0543 - val_acc: 0.7917\n",
            "Epoch 57/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0238 - acc: 0.9062 - val_loss: 0.0585 - val_acc: 0.7917\n",
            "Epoch 58/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0216 - acc: 0.8854 - val_loss: 0.0558 - val_acc: 0.7500\n",
            "Epoch 59/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0238 - acc: 0.9062 - val_loss: 0.0473 - val_acc: 0.8333\n",
            "Epoch 60/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0204 - acc: 0.9167 - val_loss: 0.0454 - val_acc: 0.8333\n",
            "Epoch 61/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0238 - acc: 0.8958 - val_loss: 0.0481 - val_acc: 0.8333\n",
            "Epoch 62/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0188 - acc: 0.9479 - val_loss: 0.0457 - val_acc: 0.8333\n",
            "Epoch 63/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0241 - acc: 0.9062 - val_loss: 0.0470 - val_acc: 0.7917\n",
            "Epoch 64/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0214 - acc: 0.9167 - val_loss: 0.0499 - val_acc: 0.8333\n",
            "Epoch 65/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0190 - acc: 0.9167 - val_loss: 0.0450 - val_acc: 0.8333\n",
            "Epoch 66/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0225 - acc: 0.9271 - val_loss: 0.0447 - val_acc: 0.8333\n",
            "Epoch 67/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0231 - acc: 0.9167 - val_loss: 0.0466 - val_acc: 0.8333\n",
            "Epoch 68/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0186 - acc: 0.9167 - val_loss: 0.0513 - val_acc: 0.8333\n",
            "Epoch 69/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0203 - acc: 0.9375 - val_loss: 0.0506 - val_acc: 0.8333\n",
            "Epoch 70/200\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0187 - acc: 0.9271 - val_loss: 0.0479 - val_acc: 0.8333\n",
            "Epoch 71/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0169 - acc: 0.9271 - val_loss: 0.0454 - val_acc: 0.8333\n",
            "Epoch 72/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0206 - acc: 0.9167 - val_loss: 0.0448 - val_acc: 0.8333\n",
            "Epoch 73/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0214 - acc: 0.9167 - val_loss: 0.0468 - val_acc: 0.8333\n",
            "Epoch 74/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0175 - acc: 0.9375 - val_loss: 0.0472 - val_acc: 0.8333\n",
            "Epoch 75/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0186 - acc: 0.9167 - val_loss: 0.0689 - val_acc: 0.6667\n",
            "Epoch 76/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0247 - acc: 0.9062 - val_loss: 0.0591 - val_acc: 0.6667\n",
            "Epoch 77/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0172 - acc: 0.9167 - val_loss: 0.0469 - val_acc: 0.7917\n",
            "Epoch 78/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0171 - acc: 0.9271 - val_loss: 0.0448 - val_acc: 0.7917\n",
            "Epoch 79/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0147 - acc: 0.9375 - val_loss: 0.0462 - val_acc: 0.8333\n",
            "Epoch 80/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0154 - acc: 0.9583 - val_loss: 0.0451 - val_acc: 0.8333\n",
            "Epoch 81/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0165 - acc: 0.9375 - val_loss: 0.0477 - val_acc: 0.8333\n",
            "Epoch 82/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0174 - acc: 0.9271 - val_loss: 0.0466 - val_acc: 0.8333\n",
            "Epoch 83/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0160 - acc: 0.9271 - val_loss: 0.0482 - val_acc: 0.8333\n",
            "Epoch 84/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0230 - acc: 0.8750 - val_loss: 0.0471 - val_acc: 0.8333\n",
            "Epoch 85/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0226 - acc: 0.9167 - val_loss: 0.0429 - val_acc: 0.8333\n",
            "Epoch 86/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0202 - acc: 0.9271 - val_loss: 0.0471 - val_acc: 0.8333\n",
            "Epoch 87/200\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0190 - acc: 0.9271 - val_loss: 0.0495 - val_acc: 0.8333\n",
            "Epoch 88/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0125 - acc: 0.9583 - val_loss: 0.0458 - val_acc: 0.8333\n",
            "Epoch 89/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0145 - acc: 0.9375 - val_loss: 0.0446 - val_acc: 0.8333\n",
            "Epoch 90/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0177 - acc: 0.9271 - val_loss: 0.0428 - val_acc: 0.8333\n",
            "Epoch 91/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0187 - acc: 0.9271 - val_loss: 0.0422 - val_acc: 0.8333\n",
            "Epoch 92/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0143 - acc: 0.9479 - val_loss: 0.0419 - val_acc: 0.8333\n",
            "Epoch 93/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0191 - acc: 0.9375 - val_loss: 0.0417 - val_acc: 0.8333\n",
            "Epoch 94/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0188 - acc: 0.9375 - val_loss: 0.0422 - val_acc: 0.8333\n",
            "Epoch 95/200\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0183 - acc: 0.9375 - val_loss: 0.0412 - val_acc: 0.8333\n",
            "Epoch 96/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0143 - acc: 0.9479 - val_loss: 0.0431 - val_acc: 0.8333\n",
            "Epoch 97/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0140 - acc: 0.9375 - val_loss: 0.0503 - val_acc: 0.8333\n",
            "Epoch 98/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0136 - acc: 0.9583 - val_loss: 0.0597 - val_acc: 0.7917\n",
            "Epoch 99/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0150 - acc: 0.9375 - val_loss: 0.0435 - val_acc: 0.8333\n",
            "Epoch 100/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0103 - acc: 0.9687 - val_loss: 0.0411 - val_acc: 0.8333\n",
            "Epoch 101/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0212 - acc: 0.9063 - val_loss: 0.0462 - val_acc: 0.8333\n",
            "Epoch 102/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0165 - acc: 0.9375 - val_loss: 0.0579 - val_acc: 0.7917\n",
            "Epoch 103/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0156 - acc: 0.9375 - val_loss: 0.0475 - val_acc: 0.8333\n",
            "Epoch 104/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0182 - acc: 0.9271 - val_loss: 0.0423 - val_acc: 0.8333\n",
            "Epoch 105/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0122 - acc: 0.9583 - val_loss: 0.0429 - val_acc: 0.8333\n",
            "Epoch 106/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0107 - acc: 0.9687 - val_loss: 0.0448 - val_acc: 0.8333\n",
            "Epoch 107/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0135 - acc: 0.9479 - val_loss: 0.0434 - val_acc: 0.8333\n",
            "Epoch 108/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0147 - acc: 0.9375 - val_loss: 0.0450 - val_acc: 0.8333\n",
            "Epoch 109/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 0.9792 - val_loss: 0.0495 - val_acc: 0.8333\n",
            "Epoch 110/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0164 - acc: 0.9479 - val_loss: 0.0470 - val_acc: 0.8333\n",
            "Epoch 111/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0132 - acc: 0.9375 - val_loss: 0.0442 - val_acc: 0.8333\n",
            "Epoch 112/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0103 - acc: 0.9583 - val_loss: 0.0429 - val_acc: 0.8333\n",
            "Epoch 113/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0157 - acc: 0.9375 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 114/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0185 - acc: 0.9062 - val_loss: 0.0438 - val_acc: 0.8333\n",
            "Epoch 115/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0146 - acc: 0.9479 - val_loss: 0.0510 - val_acc: 0.8333\n",
            "Epoch 116/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0120 - acc: 0.9479 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 117/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0071 - acc: 0.9792 - val_loss: 0.0419 - val_acc: 0.8333\n",
            "Epoch 118/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0103 - acc: 0.9687 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 119/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0105 - acc: 0.9687 - val_loss: 0.0576 - val_acc: 0.7917\n",
            "Epoch 120/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0159 - acc: 0.9479 - val_loss: 0.0464 - val_acc: 0.8333\n",
            "Epoch 121/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0075 - acc: 0.9896 - val_loss: 0.0443 - val_acc: 0.8333\n",
            "Epoch 122/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0140 - acc: 0.9479 - val_loss: 0.0453 - val_acc: 0.8333\n",
            "Epoch 123/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0119 - acc: 0.9479 - val_loss: 0.0443 - val_acc: 0.8333\n",
            "Epoch 124/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0073 - acc: 0.9792 - val_loss: 0.0421 - val_acc: 0.8333\n",
            "Epoch 125/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0141 - acc: 0.9375 - val_loss: 0.0429 - val_acc: 0.8333\n",
            "Epoch 126/200\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0115 - acc: 0.9583 - val_loss: 0.0424 - val_acc: 0.8333\n",
            "Epoch 127/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 0.9583 - val_loss: 0.0446 - val_acc: 0.8333\n",
            "Epoch 128/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0086 - acc: 0.9792 - val_loss: 0.0457 - val_acc: 0.8333\n",
            "Epoch 129/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0141 - acc: 0.9479 - val_loss: 0.0454 - val_acc: 0.8333\n",
            "Epoch 130/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0113 - acc: 0.9687 - val_loss: 0.0426 - val_acc: 0.8333\n",
            "Epoch 131/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0154 - acc: 0.9479 - val_loss: 0.0440 - val_acc: 0.8333\n",
            "Epoch 132/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0112 - acc: 0.9479 - val_loss: 0.0469 - val_acc: 0.8333\n",
            "Epoch 133/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0138 - acc: 0.9375 - val_loss: 0.0453 - val_acc: 0.8333\n",
            "Epoch 134/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0084 - acc: 0.9687 - val_loss: 0.0446 - val_acc: 0.8333\n",
            "Epoch 135/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0159 - acc: 0.9375 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 136/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0106 - acc: 0.9583 - val_loss: 0.0448 - val_acc: 0.8333\n",
            "Epoch 137/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 0.9687 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 138/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0082 - acc: 0.9687 - val_loss: 0.0434 - val_acc: 0.8333\n",
            "Epoch 139/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0082 - acc: 0.9792 - val_loss: 0.0448 - val_acc: 0.8333\n",
            "Epoch 140/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0119 - acc: 0.9687 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 141/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 0.9583 - val_loss: 0.0440 - val_acc: 0.8333\n",
            "Epoch 142/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0058 - acc: 0.9792 - val_loss: 0.0441 - val_acc: 0.8333\n",
            "Epoch 143/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0110 - acc: 0.9583 - val_loss: 0.0441 - val_acc: 0.8333\n",
            "Epoch 144/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0049 - acc: 0.9896 - val_loss: 0.0446 - val_acc: 0.8333\n",
            "Epoch 145/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 0.9896 - val_loss: 0.0453 - val_acc: 0.8333\n",
            "Epoch 146/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 0.9583 - val_loss: 0.0481 - val_acc: 0.8333\n",
            "Epoch 147/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0108 - acc: 0.9375 - val_loss: 0.0485 - val_acc: 0.8333\n",
            "Epoch 148/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0095 - acc: 0.9687 - val_loss: 0.0452 - val_acc: 0.8333\n",
            "Epoch 149/200\n",
            "96/96 [==============================] - 0s 2ms/step - loss: 0.0108 - acc: 0.9687 - val_loss: 0.0440 - val_acc: 0.8333\n",
            "Epoch 150/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0051 - acc: 0.9792 - val_loss: 0.0441 - val_acc: 0.8333\n",
            "Epoch 151/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0046 - acc: 0.9896 - val_loss: 0.0448 - val_acc: 0.8333\n",
            "Epoch 152/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0081 - acc: 0.9687 - val_loss: 0.0463 - val_acc: 0.8333\n",
            "Epoch 153/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0045 - acc: 0.9896 - val_loss: 0.0454 - val_acc: 0.8333\n",
            "Epoch 154/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0064 - acc: 0.9792 - val_loss: 0.0457 - val_acc: 0.8333\n",
            "Epoch 155/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0062 - acc: 0.9896 - val_loss: 0.0459 - val_acc: 0.8333\n",
            "Epoch 156/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 0.9792 - val_loss: 0.0456 - val_acc: 0.8333\n",
            "Epoch 157/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.9687 - val_loss: 0.0442 - val_acc: 0.8333\n",
            "Epoch 158/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0133 - acc: 0.9479 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 159/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0076 - acc: 0.9792 - val_loss: 0.0452 - val_acc: 0.8333\n",
            "Epoch 160/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0081 - acc: 0.9792 - val_loss: 0.0468 - val_acc: 0.8333\n",
            "Epoch 161/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0175 - acc: 0.9271 - val_loss: 0.0419 - val_acc: 0.8333\n",
            "Epoch 162/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0070 - acc: 0.9792 - val_loss: 0.0425 - val_acc: 0.8333\n",
            "Epoch 163/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0048 - acc: 0.9896 - val_loss: 0.0439 - val_acc: 0.8333\n",
            "Epoch 164/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0076 - acc: 0.9792 - val_loss: 0.0465 - val_acc: 0.8333\n",
            "Epoch 165/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 0.9687 - val_loss: 0.0481 - val_acc: 0.8333\n",
            "Epoch 166/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0090 - acc: 0.9687 - val_loss: 0.0459 - val_acc: 0.8333\n",
            "Epoch 167/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0083 - acc: 0.9687 - val_loss: 0.0451 - val_acc: 0.8333\n",
            "Epoch 168/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 0.9687 - val_loss: 0.0451 - val_acc: 0.8333\n",
            "Epoch 169/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0045 - acc: 0.9896 - val_loss: 0.0464 - val_acc: 0.8333\n",
            "Epoch 170/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0049 - acc: 0.9896 - val_loss: 0.0458 - val_acc: 0.8333\n",
            "Epoch 171/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0066 - acc: 0.9792 - val_loss: 0.0448 - val_acc: 0.8333\n",
            "Epoch 172/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 0.9687 - val_loss: 0.0449 - val_acc: 0.8333\n",
            "Epoch 173/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 0.9687 - val_loss: 0.0461 - val_acc: 0.8333\n",
            "Epoch 174/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0082 - acc: 0.9687 - val_loss: 0.0464 - val_acc: 0.8333\n",
            "Epoch 175/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0047 - acc: 0.9896 - val_loss: 0.0461 - val_acc: 0.8333\n",
            "Epoch 176/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0099 - acc: 0.9687 - val_loss: 0.0511 - val_acc: 0.8333\n",
            "Epoch 177/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0053 - acc: 0.9896 - val_loss: 0.0451 - val_acc: 0.8333\n",
            "Epoch 178/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 0.9583 - val_loss: 0.0421 - val_acc: 0.8333\n",
            "Epoch 179/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0046 - acc: 0.9896 - val_loss: 0.0421 - val_acc: 0.8333\n",
            "Epoch 180/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0069 - acc: 0.9792 - val_loss: 0.0456 - val_acc: 0.8333\n",
            "Epoch 181/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0091 - acc: 0.9687 - val_loss: 0.0481 - val_acc: 0.7917\n",
            "Epoch 182/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0044 - acc: 0.9792 - val_loss: 0.0509 - val_acc: 0.7917\n",
            "Epoch 183/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0054 - acc: 0.9792 - val_loss: 0.0563 - val_acc: 0.7500\n",
            "Epoch 184/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0084 - acc: 0.9583 - val_loss: 0.0474 - val_acc: 0.8333\n",
            "Epoch 185/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0048 - acc: 0.9896 - val_loss: 0.0420 - val_acc: 0.8333\n",
            "Epoch 186/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0065 - acc: 0.9792 - val_loss: 0.0423 - val_acc: 0.8333\n",
            "Epoch 187/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0053 - acc: 0.9792 - val_loss: 0.0460 - val_acc: 0.8333\n",
            "Epoch 188/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0040 - acc: 0.9896 - val_loss: 0.0475 - val_acc: 0.8333\n",
            "Epoch 189/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0067 - acc: 0.9792 - val_loss: 0.0481 - val_acc: 0.8333\n",
            "Epoch 190/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0077 - acc: 0.9583 - val_loss: 0.0458 - val_acc: 0.8333\n",
            "Epoch 191/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0053 - acc: 0.9792 - val_loss: 0.0454 - val_acc: 0.8333\n",
            "Epoch 192/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0050 - acc: 0.9896 - val_loss: 0.0457 - val_acc: 0.8333\n",
            "Epoch 193/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0065 - acc: 0.9792 - val_loss: 0.0460 - val_acc: 0.8333\n",
            "Epoch 194/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0079 - acc: 0.9687 - val_loss: 0.0467 - val_acc: 0.8333\n",
            "Epoch 195/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0099 - acc: 0.9583 - val_loss: 0.0459 - val_acc: 0.8333\n",
            "Epoch 196/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0080 - acc: 0.9687 - val_loss: 0.0463 - val_acc: 0.8333\n",
            "Epoch 197/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0103 - acc: 0.9479 - val_loss: 0.0462 - val_acc: 0.8333\n",
            "Epoch 198/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0059 - acc: 0.9792 - val_loss: 0.0454 - val_acc: 0.8333\n",
            "Epoch 199/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0069 - acc: 0.9792 - val_loss: 0.0454 - val_acc: 0.8333\n",
            "Epoch 200/200\n",
            "96/96 [==============================] - 0s 1ms/step - loss: 0.0054 - acc: 0.9896 - val_loss: 0.0451 - val_acc: 0.8333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUXFXFEUpQv4/2ekd9BjSAvAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "['frog.npy', 'crow.npy', 'crickets.npy']\n",
            "[[9.9198687e-01 7.4935886e-03 5.1948044e-04]]\n",
            "[[1.6548749e-04 9.9911684e-01 7.1773346e-04]]\n",
            "[[3.97958161e-07 1.14893755e-05 9.99988079e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd22-wo38PdI",
        "colab_type": "code",
        "outputId": "3bb9d615-90a0-451a-dbf0-f37a7f402856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "eval_speech_graph = tf.Graph()\n",
        "eval_speech_sess = tf.Session(graph=eval_speech_graph)\n",
        "\n",
        "keras.backend.set_session(eval_speech_sess)\n",
        "with eval_speech_graph.as_default():\n",
        "    keras.backend.set_learning_phase(0)\n",
        "    eval_model = build_speech_model()\n",
        "\n",
        "    #For quantization aware training only\n",
        "    #tf.contrib.quantize.create_eval_graph(input_graph=eval_speech_graph)\n",
        "    eval_speech_graph_def = eval_speech_graph.as_graph_def()\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(eval_speech_sess, 'checkpoints')\n",
        "\n",
        "    frozen_graph_def = tf.graph_util.convert_variables_to_constants( eval_speech_sess, eval_speech_graph_def, \n",
        "                                                                    [eval_model.output.op.name] )\n",
        "\n",
        "    with open('frozen_urban_model.pb', 'wb') as f:\n",
        "      f.write(frozen_graph_def.SerializeToString())\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from checkpoints\n",
            "INFO:tensorflow:Froze 10 variables.\n",
            "INFO:tensorflow:Converted 10 variables to const ops.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D8IB8J1shCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMyYjW5yrKK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.enable_eager_execution()\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_frozen_graph( \"frozen_urban_model.pb\", [\"conv2d_1_input\"], [\"dense_2/Softmax\"])\n",
        "tflite_model = converter.convert()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLO2TkGwsBeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tflite_models_dir = pathlib.Path(\"/tmp/urban_sound_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RitjVD2Js0V3",
        "colab_type": "code",
        "outputId": "c8f0b3ba-6619-4a3e-9656-014d72bdef09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Obtaining a tflite model \n",
        "\n",
        "tflite_model_file = tflite_models_dir/\"urban_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66240"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crknIo-JYrx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8af5f8df-7ada-4a8c-f9ee-646aeb5a5ab9"
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 68K\n",
            "-rw-r--r-- 1 root root 65K Oct  1 05:54 urban_model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGE056pDtivJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimize for size\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4UCNK2_us8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create quantized values with an accurate dynamic range of activations, \n",
        "# for that need to provide a representative dataset\n",
        "\n",
        "sounds = tf.cast(X_train, tf.float32)/255.0\n",
        "urban_ds = tf.data.Dataset.from_tensor_slices((sounds)).batch(1)\n",
        "def representative_data_gen():\n",
        "  for input_value in urban_ds.take(100):\n",
        "    yield [input_value]\n",
        "    \n",
        "converter.representative_dataset = representative_data_gen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAoR5sRSw-k-",
        "colab_type": "code",
        "outputId": "2b5343c0-de10-463f-800d-210827b10da5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"urban_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_quant_model)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25792"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YWSytIAykKh",
        "colab_type": "code",
        "outputId": "76f7094a-f1f5-4d2f-f380-a173f9dbc3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 96K\n",
            "-rw-r--r-- 1 root root 26K Oct  1 05:54 urban_model_quant.tflite\n",
            "-rw-r--r-- 1 root root 65K Oct  1 05:54 urban_model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZBZr43SXFT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37176e7d-075e-4ac9-ef2e-71a772c07941"
      },
      "source": [
        "# The converted model needs to be fully quantized. That means all ops need to be \n",
        "# quantized, no floats left. The input and outputs need to be integers too.\n",
        "\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()\n",
        "tflite_model_quant_file = tflite_models_dir/\"urban_model_quant_io.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25864"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7X_N6u2ZEOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "be495e54-ccf4-4e24-f2ac-e60996bc0624"
      },
      "source": [
        "!ls -lh {tflite_models_dir}"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 124K\n",
            "-rw-r--r-- 1 root root 26K Oct  1 05:54 urban_model_quant_io.tflite\n",
            "-rw-r--r-- 1 root root 26K Oct  1 05:54 urban_model_quant.tflite\n",
            "-rw-r--r-- 1 root root 65K Oct  1 05:54 urban_model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPOtEZ6zahdp",
        "colab_type": "text"
      },
      "source": [
        "### Loading test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1INga5Uad9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data for float model\n",
        "sounds = tf.cast(X_test, tf.float32)/255.0\n",
        "urban_ds = tf.data.Dataset.from_tensor_slices(sounds).batch(1)\n",
        "\n",
        "# Load data for quantized model\n",
        "sounds_uint8 = tf.cast(X_test, tf.uint8)\n",
        "urban_ds_uint8 = tf.data.Dataset.from_tensor_slices(sounds_uint8).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3MUe1MqCXqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crow_uint8 = tf.cast(crow_file, tf.uint8)\n",
        "frog_uint8 = tf.cast(frog_file, tf.uint8)\n",
        "crickets_uint8 = tf.cast(crickets_file, tf.uint8)\n",
        "print(crow_uint8.shape)\n",
        "print(crickets_uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT_yi2LbDTav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save('/content/' + 'crow_single_uint8.npy', crow_uint8)\n",
        "np.save('/content/' + 'frog_single_uint8.npy', frog_uint8)\n",
        "np.save('/content/' + 'crickets_single_uint8.npy', crickets_uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZh1C9enaeLb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3ea43298-0e7b-4896-da90-01e5fd1fd45a"
      },
      "source": [
        "print(sounds_uint8.shape)\n",
        "print(sounds_uint8)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24, 10, 51, 1)\n",
            "tf.Tensor(\n",
            "[[[[248]\n",
            "   [ 36]\n",
            "   [ 42]\n",
            "   ...\n",
            "   [222]\n",
            "   [202]\n",
            "   [181]]\n",
            "\n",
            "  [[ 14]\n",
            "   [247]\n",
            "   [238]\n",
            "   ...\n",
            "   [  3]\n",
            "   [  7]\n",
            "   [ 16]]\n",
            "\n",
            "  [[218]\n",
            "   [216]\n",
            "   [228]\n",
            "   ...\n",
            "   [250]\n",
            "   [252]\n",
            "   [ 12]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[247]\n",
            "   [235]\n",
            "   [241]\n",
            "   ...\n",
            "   [239]\n",
            "   [239]\n",
            "   [237]]\n",
            "\n",
            "  [[240]\n",
            "   [243]\n",
            "   [  4]\n",
            "   ...\n",
            "   [244]\n",
            "   [239]\n",
            "   [238]]\n",
            "\n",
            "  [[  3]\n",
            "   [ 10]\n",
            "   [ 29]\n",
            "   ...\n",
            "   [ 16]\n",
            "   [ 25]\n",
            "   [  5]]]\n",
            "\n",
            "\n",
            " [[[217]\n",
            "   [249]\n",
            "   [  0]\n",
            "   ...\n",
            "   [148]\n",
            "   [145]\n",
            "   [131]]\n",
            "\n",
            "  [[ 50]\n",
            "   [  1]\n",
            "   [235]\n",
            "   ...\n",
            "   [ 21]\n",
            "   [ 30]\n",
            "   [ 31]]\n",
            "\n",
            "  [[214]\n",
            "   [164]\n",
            "   [163]\n",
            "   ...\n",
            "   [188]\n",
            "   [191]\n",
            "   [202]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  3]\n",
            "   [254]\n",
            "   [251]\n",
            "   ...\n",
            "   [  5]\n",
            "   [ 11]\n",
            "   [ 21]]\n",
            "\n",
            "  [[241]\n",
            "   [243]\n",
            "   [243]\n",
            "   ...\n",
            "   [245]\n",
            "   [254]\n",
            "   [245]]\n",
            "\n",
            "  [[  2]\n",
            "   [ 14]\n",
            "   [ 28]\n",
            "   ...\n",
            "   [  9]\n",
            "   [ 15]\n",
            "   [  3]]]\n",
            "\n",
            "\n",
            " [[[191]\n",
            "   [155]\n",
            "   [128]\n",
            "   ...\n",
            "   [142]\n",
            "   [134]\n",
            "   [ 55]]\n",
            "\n",
            "  [[ 77]\n",
            "   [154]\n",
            "   [146]\n",
            "   ...\n",
            "   [ 92]\n",
            "   [ 95]\n",
            "   [ 48]]\n",
            "\n",
            "  [[199]\n",
            "   [128]\n",
            "   [113]\n",
            "   ...\n",
            "   [118]\n",
            "   [114]\n",
            "   [184]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[248]\n",
            "   [  8]\n",
            "   [251]\n",
            "   ...\n",
            "   [ 17]\n",
            "   [ 11]\n",
            "   [  7]]\n",
            "\n",
            "  [[ 15]\n",
            "   [ 26]\n",
            "   [ 22]\n",
            "   ...\n",
            "   [  2]\n",
            "   [255]\n",
            "   [235]]\n",
            "\n",
            "  [[  8]\n",
            "   [  6]\n",
            "   [  1]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [240]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 84]\n",
            "   [ 98]\n",
            "   [118]\n",
            "   ...\n",
            "   [110]\n",
            "   [119]\n",
            "   [110]]\n",
            "\n",
            "  [[ 66]\n",
            "   [ 55]\n",
            "   [ 51]\n",
            "   ...\n",
            "   [ 67]\n",
            "   [ 74]\n",
            "   [ 78]]\n",
            "\n",
            "  [[214]\n",
            "   [217]\n",
            "   [226]\n",
            "   ...\n",
            "   [193]\n",
            "   [183]\n",
            "   [201]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  3]\n",
            "   [ 10]\n",
            "   [255]\n",
            "   ...\n",
            "   [251]\n",
            "   [  3]\n",
            "   [ 13]]\n",
            "\n",
            "  [[246]\n",
            "   [  0]\n",
            "   [251]\n",
            "   ...\n",
            "   [  0]\n",
            "   [253]\n",
            "   [  4]]\n",
            "\n",
            "  [[  0]\n",
            "   [ 11]\n",
            "   [ 11]\n",
            "   ...\n",
            "   [  6]\n",
            "   [  9]\n",
            "   [  1]]]\n",
            "\n",
            "\n",
            " [[[178]\n",
            "   [206]\n",
            "   [250]\n",
            "   ...\n",
            "   [161]\n",
            "   [103]\n",
            "   [ 93]]\n",
            "\n",
            "  [[ 69]\n",
            "   [ 31]\n",
            "   [ 26]\n",
            "   ...\n",
            "   [ 37]\n",
            "   [ 38]\n",
            "   [ 42]]\n",
            "\n",
            "  [[140]\n",
            "   [ 95]\n",
            "   [ 92]\n",
            "   ...\n",
            "   [103]\n",
            "   [134]\n",
            "   [152]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 32]\n",
            "   [ 57]\n",
            "   [ 47]\n",
            "   ...\n",
            "   [ 49]\n",
            "   [ 39]\n",
            "   [ 53]]\n",
            "\n",
            "  [[  4]\n",
            "   [  4]\n",
            "   [ 14]\n",
            "   ...\n",
            "   [250]\n",
            "   [  0]\n",
            "   [ 18]]\n",
            "\n",
            "  [[224]\n",
            "   [235]\n",
            "   [242]\n",
            "   ...\n",
            "   [219]\n",
            "   [234]\n",
            "   [238]]]\n",
            "\n",
            "\n",
            " [[[ 92]\n",
            "   [ 79]\n",
            "   [ 90]\n",
            "   ...\n",
            "   [105]\n",
            "   [102]\n",
            "   [ 93]]\n",
            "\n",
            "  [[ 56]\n",
            "   [ 68]\n",
            "   [ 73]\n",
            "   ...\n",
            "   [ 20]\n",
            "   [ 23]\n",
            "   [ 30]]\n",
            "\n",
            "  [[206]\n",
            "   [184]\n",
            "   [181]\n",
            "   ...\n",
            "   [165]\n",
            "   [169]\n",
            "   [179]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 17]\n",
            "   [ 26]\n",
            "   [ 18]\n",
            "   ...\n",
            "   [ 14]\n",
            "   [  8]\n",
            "   [ 12]]\n",
            "\n",
            "  [[ 23]\n",
            "   [ 14]\n",
            "   [  6]\n",
            "   ...\n",
            "   [  6]\n",
            "   [  1]\n",
            "   [  1]]\n",
            "\n",
            "  [[  6]\n",
            "   [ 17]\n",
            "   [ 11]\n",
            "   ...\n",
            "   [  0]\n",
            "   [254]\n",
            "   [247]]]], shape=(24, 10, 51, 1), dtype=uint8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6PawVdndeU9",
        "colab_type": "text"
      },
      "source": [
        "### use model in tflite interpreter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPjZZtqVaeUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUx2J6qFaedz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "interpreter_quant.allocate_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XT-CiIuaend",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5983fa84-d45f-4dd9-a814-f92e8e245d10"
      },
      "source": [
        "for sound in urban_ds:\n",
        "  break\n",
        "\n",
        "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], sound)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor(\n",
        "    interpreter.get_output_details()[0][\"index\"])\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.28681096 0.450979   0.26221004]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LLWWQxVgUDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eda9b280-520f-4663-906f-faff54de4750"
      },
      "source": [
        "for sound in urban_ds_uint8:\n",
        "  break\n",
        "\n",
        "interpreter_quant.set_tensor(\n",
        "    interpreter_quant.get_input_details()[0][\"index\"], sound)\n",
        "interpreter_quant.invoke()\n",
        "predictions = interpreter_quant.get_tensor(\n",
        "    interpreter_quant.get_output_details()[0][\"index\"])\n",
        "\n",
        "print(predictions)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 67 123  66]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7hP3SdSg33g",
        "colab_type": "text"
      },
      "source": [
        "### How is my model doing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSClZKTig3iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(interpreter, urban_ds):\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  for sound in mnist_ds:\n",
        "    total_seen += 1\n",
        "    interpreter.set_tensor(input_index, sound)\n",
        "    interpreter.invoke()\n",
        "    predictions = interpreter.get_tensor(output_index)\n",
        "    if predictions == label.numpy():\n",
        "      num_correct += 1\n",
        "\n",
        "    if total_seen % 500 == 0:\n",
        "      print(\"Accuracy after %i images: %f\" %\n",
        "            (total_seen, float(num_correct) / float(total_seen)))\n",
        "\n",
        "  return float(num_correct) / float(total_seen)\n",
        "\n",
        "urban_ds_demo = urban_ds.take(10)\n",
        "print(eval_model(interpreter, urban_ds_demo))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyaiwwqXy6JH",
        "colab_type": "text"
      },
      "source": [
        "### Running the TFLite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzs8DCQxzANt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sounds_test = tf.cast(X_test, tf.float32)/255.0\n",
        "#sounds_test = tf.dtypes.cast(X_test, tf.int8)/255.0\n",
        "sounds_test = tf.dtypes.cast(X_test, tf.int8) \n",
        "urban_test_ds = tf.data.Dataset.from_tensor_slices(sounds_test).batch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87-Yxh_Senwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sounds_test.shape)\n",
        "print(sounds_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5tO2z-l0Ib5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
        "interpreter.allocate_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBKy6tcf0hAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interpreter_quant = tf.lite.Interpreter(model_path=str(tflite_model_quant_file))\n",
        "interpreter_quant.allocate_tensors()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVJGn5jE02ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sound in urban_test_ds:\n",
        "  break\n",
        "  \n",
        "interpreter.set_tensor(interpreter.get_input_details()[0][\"index\"], sound)\n",
        "interpreter.invoke()\n",
        "predictions = interpreter.get_tensor (interpreter.get_output_details()[0][\"index\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_A5P3ej1mBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWo-7FDpc_3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade tensorflow\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}