{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_train.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villasen/colab_notebooks/blob/master/Keras_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XfXFsAnRkn9",
        "colab_type": "text"
      },
      "source": [
        "## Urban dataset download and file structure setup for train, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6mpdiRj4-Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/villasen/Sound-Datasets.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQz67Mm9GIUy",
        "colab_type": "code",
        "outputId": "662b7ad7-c45b-4ef9-dc96-3a47cdfa3475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import librosa\n",
        "import os, shutil"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okrUJ3UKDo2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_dir = '/content/target_dataset'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOS3_a7TAW-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip combined_datasets.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7jltmUYDZGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r combined_datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMuCJzsVTh0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%env URBAN_WORD1=church_bells\n",
        "%env URBAN_WORD2=gun_shot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW2q1YWatty_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r /content/target_dataset/train \n",
        "!rm -r /content/target_dataset/validation\n",
        "!rm -r /content/target_dataset/test\n",
        "!rm -r /content/target_dataset/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHBULmNeFteL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "base_directory ='/content/target_dataset/'\n",
        "os.mkdir(base_directory)\n",
        "train_dir = os.path.join(base_directory, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_directory, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_directory, 'test')\n",
        "os.mkdir(test_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDMMKPgkXSko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_dataset_dir = '/content/Sound-Datasets/combined_datasets/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcURx0euURiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_gunshot_dir = os.path.join(train_dir, 'gun_shot')\n",
        "os.mkdir(train_gunshot_dir)\n",
        "train_dogbark_dir = os.path.join(train_dir, 'dog_bark')\n",
        "os.mkdir(train_dogbark_dir)\n",
        "\n",
        "validation_gunshot_dir = os.path.join(validation_dir, 'gun_shot')\n",
        "os.mkdir(validation_gunshot_dir)\n",
        "validation_dogbark_dir = os.path.join(validation_dir, 'dog_bark')\n",
        "os.mkdir(validation_dogbark_dir)\n",
        "\n",
        "test_gunshot_dir = os.path.join(test_dir, 'gun_shot')\n",
        "os.mkdir(test_gunshot_dir)\n",
        "test_dogbark_dir = os.path.join(test_dir, 'dog_bark')\n",
        "os.mkdir(test_dogbark_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ-fuVCQZWWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fnames_dog = os.listdir('/content/Sound-Datasets/combined_datasets/dog_bark')\n",
        "fnames_gun = os.listdir('/content/Sound-Datasets/combined_datasets/gun_shot')\n",
        "fnames = os.listdir('/content/Sound-Datasets/combined_datasets/dog_bark')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6z3vWiXhtRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(fnames_dog)\n",
        "print(fnames_gun)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54t8veJLZdw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(fnames)\n",
        "size = len(fnames)\n",
        "train_percentage = 0.8\n",
        "validation_percentage = 0.1\n",
        "test_percentage = 0.1\n",
        "print('train percentage = %f' %train_percentage)\n",
        "train_size = int(round(size * (train_percentage)))\n",
        "validation_size = int(round(size * (validation_percentage)))\n",
        "test_size = int(round(size * (test_percentage)))\n",
        "print(size)\n",
        "print('train_size = %i' %train_size)\n",
        "print('validation_size = %i' %validation_size)\n",
        "print('test_size = %i' %test_size)\n",
        "total = train_size + validation_size + test_size\n",
        "print(total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuY-y6gKrvNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(fnames))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pojp7QvbfKch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "#os.chdir('/content/combined_datasets/dog_bark')\n",
        "train_list=[]\n",
        "validation_list=[]\n",
        "test_list=[]\n",
        "\n",
        "train_list = fnames[0:train_size]\n",
        "validation_list = fnames[train_size:train_size+validation_size]\n",
        "test_list = fnames[train_size+validation_size:total]\n",
        "\n",
        "\n",
        "for train in train_list:\n",
        "  src_train = os.path.join(original_dataset_dir+'dog_bark', train )\n",
        "  dst_train = os.path.join('/content/target_dataset/train/dog_bark', train)\n",
        "  shutil.copyfile(src_train, dst_train)\n",
        "\n",
        "for validation in validation_list:\n",
        "  src_validation = os.path.join(original_dataset_dir+'dog_bark', validation)\n",
        "  dst_validation = os.path.join('/content/target_dataset/validation/dog_bark', validation)\n",
        "  shutil.copyfile(src_validation, dst_validation)\n",
        "  \n",
        "for test in test_list:\n",
        "  src_test = os.path.join(original_dataset_dir+'dog_bark', test)\n",
        "  dst_test = os.path.join('/content/target_dataset/test/dog_bark', test)\n",
        "  shutil.copyfile(src_test, dst_test)\n",
        "  \n",
        "        \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGHbIj_342dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_list)\n",
        "print(len(train_list))\n",
        "print(validation_list)\n",
        "print(len(validation_list))\n",
        "print(test_list)\n",
        "print(len(test_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOB0kg2_DGCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fnames_train = os.listdir('/content/target_dataset/train/dog_bark')\n",
        "print(len(fnames_train))\n",
        "fnames_val = os.listdir('/content/target_dataset/validation/dog_bark')\n",
        "print(len(fnames_val))\n",
        "fnames_test = os.listdir('/content/target_dataset/test/dog_bark')\n",
        "print(len(fnames_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z94KvPBWU6kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validation_gunshot_dir = os.path.join(validation_dir, 'gun_shot')\n",
        "os.mkdir(validation_gunshot_dir)\n",
        "validation_dogbark_dir = os.path.join(validation_dir, 'dog_bark')\n",
        "os.mkdir(validation_dogbark_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imuKEMG8U8Jg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_gunshot_dir = os.path.join(test_dir, 'gun_shot')\n",
        "os.mkdir(test_gunshot_dir)\n",
        "test_dogbark_dir = os.path.join(test_dir, 'dog_bark')\n",
        "os.mkdir(test_dogbark_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywdXdFQFRUXu",
        "colab_type": "text"
      },
      "source": [
        "###Preparing for google speech dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdio_TpuAE5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_url='http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "os.makedirs(dest_directory)\n",
        "filename = data_url.split('/')[-1]\n",
        "filepath = os.path.join(dest_directory, filename)\n",
        "\n",
        "print(filename)\n",
        "print(filepath)\n",
        " \n",
        "def _progress(count, block_size, total_size):\n",
        "    sys.stdout.write('\\r>> Downloading %s %.1f%%' % \\\n",
        "            (filename, float(count * block_size) / float(total_size) * 100.0)) \n",
        "    sys.stdout.flush()\n",
        "\n",
        "urban_dataset_dir = '/content/combined_datasets/' \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z1f7OCcESAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath, _ = urllib.request.urlretrieve(data_url, filepath, _progress)\n",
        "statinfo = os.stat(filepath)\n",
        "tf.logging.info('Successfully downloaded %s (%d bytes)', filename, statinfo.st_size)\n",
        "tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRonIfwSUo13",
        "colab_type": "text"
      },
      "source": [
        "### Converting WAV files to mfcc coefficients stored in numpy array\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHImY4LaKGmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wav2mfcc(file_path, max_pad_len=)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWZNjHYTtLnS",
        "colab_type": "text"
      },
      "source": [
        "# Building DS-CNN model using Keras framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh0hc37amv70",
        "colab_type": "text"
      },
      "source": [
        "### Building the DS_CNN Network\n",
        "This network containst 5 layers. First layer is a convolutional neural network and the remaining layers are depthwise separable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWsjPl-K0IE4",
        "colab_type": "code",
        "outputId": "bc6cdd50-acf2-4dd9-9cac-d26bd9028c97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Creating Keras sequential model\n",
        "#bn = 1\n",
        "BN=False\n",
        "model = models.Sequential()\n",
        "\n",
        "def dscnn_train():\n",
        "  # 1\n",
        "      model.add(layers.Conv2D(64, (4,10), strides=(2,2), padding='same', activation='relu', \\\n",
        "                input_shape=(10, 49, 1)))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                                center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                                gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                                moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                                gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      # 2\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))  \n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      # 3\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "\n",
        "      # 4\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "\n",
        "      # 5\n",
        "      model.add(layers.SeparableConv2D(64, (3,3), strides=(1,1), data_format='channels_last', padding='same', depth_multiplier=1, activation='relu'))\n",
        "      if BN == True:\n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      model.add(layers.Conv2D(64, (1,1), strides=(1,1), padding='same', use_bias=False))\n",
        "      if BN == True:      \n",
        "          model.add(layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, \\\n",
        "                                              center=True, scale=True, beta_initializer='zeros', \\\n",
        "                                              gamma_initializer='ones', moving_mean_initializer='zeros', \\\n",
        "                                              moving_variance_initializer='ones', beta_regularizer=None, \\\n",
        "                                              gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
        "      model.add(layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0))\n",
        "\n",
        "      # Final layer\n",
        "      model.add(layers.AveragePooling2D(pool_size=(5, 25), strides=(2,2), padding='valid', data_format=None))\n",
        "      model.add(layers.Flatten(data_format=None))\n",
        "      model.add(layers.Dense(64, activation='relu'))\n",
        "      model.add(layers.Dense(10, activation='softmax'))\n",
        "  \n",
        "      # Compilation step to choose loss function, optimizer and metric\n",
        "      # Configuring the learning process\n",
        "      model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "      #model.fit()\n",
        "      #model.fit(X_train, y_train_hot, batch_size=100, epochs=200, verbose=1, validation_data=(X_test, y_test_hot))\n",
        "  \n",
        "      # Restarts layer sequence number \n",
        "      K.clear_session()\n",
        "\n",
        "  \n",
        "  \n",
        "dscnn_train()  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiLUN--y0u79",
        "colab_type": "code",
        "outputId": "d98c9ace-d933-4ad8-c932-d6f4c5107d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 5, 25, 64)         2624      \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_1 (Separabl (None, 5, 25, 64)         4736      \n",
            "_________________________________________________________________\n",
            "re_lu_2 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 5, 25, 64)         4096      \n",
            "_________________________________________________________________\n",
            "re_lu_3 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_2 (Separabl (None, 5, 25, 64)         4736      \n",
            "_________________________________________________________________\n",
            "re_lu_4 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 5, 25, 64)         4096      \n",
            "_________________________________________________________________\n",
            "re_lu_5 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_3 (Separabl (None, 5, 25, 64)         4736      \n",
            "_________________________________________________________________\n",
            "re_lu_6 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 5, 25, 64)         4096      \n",
            "_________________________________________________________________\n",
            "re_lu_7 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_4 (Separabl (None, 5, 25, 64)         4736      \n",
            "_________________________________________________________________\n",
            "re_lu_8 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 5, 25, 64)         4096      \n",
            "_________________________________________________________________\n",
            "re_lu_9 (ReLU)               (None, 5, 25, 64)         0         \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 42,762\n",
            "Trainable params: 42,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}